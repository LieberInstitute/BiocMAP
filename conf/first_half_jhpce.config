//=============================================================================
//  Hardware and software configuration for pipeline execution at JHPCE
//=============================================================================

env.OMP_NUM_THREADS = 2

params {    
    //-------------------------------------------------------------------------
    //  Annotation-related settings
    //-------------------------------------------------------------------------
    
    gencode_version_human = "34"
    gencode_version_mouse = "M23"
    anno_build = "main" // main or primary (main is canonical seqs only)
    
    //-------------------------------------------------------------------------
    //  Arioc alignment settings; see the following for more info:
    //  
    //    -Arioc docs: https://github.com/RWilton/Arioc/blob/master/Arioc.guide.pdf
    //    -our docs: http://research.libd.org/WGBS-Pipeline/configuration.html#arioc-settings
    //-------------------------------------------------------------------------
    
    // The partition containing GPU access for use with Arioc
    arioc_queue = "gpu"

    // Number of reads per batch per GPU: this scales closely with the amount
    // of GPU memory that will be required, and thus will be GPU-dependent
    batch_size = "128k"
    
    // Different seed widths and hash-value sizes can be set here, potentially
    // affecting alignment sensitivity and GPU memory requirements. Arioc has a
    // "gapped" and "nongapped" lookup table, which each have an associated
    // seed.
    gapped_seed = "hsi25_0_30_CT"
    nongapped_seed = "ssi84_2_30_CT"
    
    // Literal arguments passed to the "gapped" and "nongapped" elements of
    // alignment configs for Arioc, respectively. Variables like "maxJ" and
    // "seedDepth" may be raised to increase sensitivity or lowered to increase
    // speed, for example.
    gapped_args = 'Wmxgs="2,6,5,3" Vt="L,0,1" maxJ="20" seedDepth="4"'
    nongapped_args = 'maxJ="200" maxMismatches="5"'
    
    // Settings that mostly can optimize performance for particular hardware:
    // for example, the "useHinGmem" variable controls whether lookup tables 
    // are placed in GPU memory vs. system memory
    x_args = 'watchdogInterval="60" cgaReserved="24M" useHinGmem="1" useJinGmem="1" useHJinGPmem="1" serialLUTinit="1"'
    
    // The number of GPUs to use for alignment of each sample. It is only
    // recommended to raise this above 1 if there are more GPUs available than
    // samples.
    max_gpus = 1
    
    // At JHPCE, SLURM or SGE makes sure that GPUs are not allocated to more
    // than one user at once, so this setting should be kept false.
    manually_set_gpu = false
    
    // The percentage of volatile GPU utilization, below which a GPU is
    // considered to be unoccupied, and therefore available to this pipeline.
    // Applicable only if 'manually_set_gpu' is true
    gpu_perc_usage_cutoff = 10
}

//-----------------------------------------------------------------------------
//  Hardware resource requirements
//-----------------------------------------------------------------------------

executor {
    name = 'slurm'
    queueSize = 40
    submitRateLimit = '1 sec'
    exitReadTimeout = '40 min'
}

process {

    cache = 'lenient'
    
    withName: PullReference {
        cpus = 1
        memory = 5.GB
    }
  
    withName: PrepareReference {
        cpus = 1
        module = 'conda_R/4.3'
        memory = 3.GB
    }
    
    withName: EncodeReference {
        cpus = 1
        module = 'arioc/1.43'
        queue = params.arioc_queue
        memory = 200.GB
        clusterOptions = '--gpus=' + params.max_gpus
    }
  
    withName: PreprocessInputs {
        cpus = 1
        module = 'conda_R/4.3'
        memory = 3.GB
    }
    
    withName: FastQC_Untrimmed {
        cpus = 2
        module = 'fastqc/0.11.8'
        memory = 10.GB
    }
    
    withName: Trimming {
        cpus = 1
        module = 'trimgalore/0.6.6'
        memory = 5.GB
    }
    
    withName: WriteAriocConfigs {
        cpus = 1
        module = 'conda_R/4.3'
        memory = 2.GB
    }
    
    withName: EncodeReads {
        cpus = 2
        module = 'arioc/1.43'
        memory = 20.GB
    }
    
    withName: AlignReads {
        cpus = 10
        module = 'arioc/1.43'
        memory = 300.GB
        queue = params.arioc_queue
        clusterOptions = '--gpus=' + params.max_gpus
    }
    
    withName: FilterAlignments {
        cpus = 2
        module = 'samtools/1.10:samblaster/0.1.26'
        memory = 16.GB
    }
    
    withName: MakeRules {
        cpus = 1
        memory = 2.GB
    }
}
