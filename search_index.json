[["index.html", "BiocMAP Overview Cite BiocMAP R session information", " BiocMAP Nicholas J. Eagles Lieber Institute for Brain Development, Johns Hopkins Medical Campus Leonardo Collado-Torres Lieber Institute for Brain Development, Johns Hopkins Medical CampusCenter for Computational Biology, Johns Hopkins University lcolladotor@gmail.com Overview BiocMAP is a pair of Nextflow-based pipelines for processing raw whole genome bisulfite sequencing data into Bioconductor-friendly bsseq objects in R. The first BiocMAP module performs speedy alignment to a reference genome by Arioc, and requires GPU resources. Methylation extraction and remaining steps are performed in the second module, optionally on a different computing system where GPUs need not be available. Diagram representing the “conceptual” workflow traversed by BiocMAP. Here some nextflow processes are grouped together for simplicity; the exact processes traversed are enumerated here. The red box indicates the FASTQ files are inputs to the pipeline; green coloring denotes major output files from the pipeline; the remaining boxes represent computational steps. Yellow-colored steps are optional or not always performed; for example, lambda pseudoalignment is an optional step intended for experiments with spike-ins of the lambda bacteriophage. Finally, blue-colored steps are ordinary processes which occur on every pipeline execution. Cite BiocMAP We hope BiocMAP will be a useful tool for your research. Please use the following bibtex information to cite this software. Thank you! @article {Eagles2022.04.20.488947, author = {Eagles, Nicholas J and Wilton, Richard and Jaffe, Andrew E. and Collado-Torres, Leonardo}, title = {BiocMAP: A Bioconductor-friendly, GPU-Accelerated Pipeline for Bisulfite-Sequencing Data}, year = {2022}, doi = {10.1101/2022.04.20.488947}, publisher = {Cold Spring Harbor Laboratory}, URL = {https://doi.org/10.1101/2022.04.20.488947}, journal = {bioRxiv} } This is a project by the R/Bioconductor-powered Team Data Science at the Lieber Institute for Brain Development. R session information Details on the R version used for making this book. The source code is available at LieberInstitute/BiocMAP. ## Load the package at the top of your script library(&quot;sessioninfo&quot;) ## Reproducibility information print(&#39;Reproducibility information:&#39;) Sys.time() proc.time() options(width = 120) session_info() ## [1] &quot;Reproducibility information:&quot; ## [1] &quot;2025-04-23 19:29:59 UTC&quot; ## user system elapsed ## 0.457 0.353 0.458 ## ─ Session info ─────────────────────────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.5.0 (2025-04-11) ## os Ubuntu 24.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz UTC ## date 2025-04-23 ## pandoc 3.6.4 @ /usr/bin/ (via rmarkdown) ## quarto 1.5.57 @ /usr/local/bin/quarto ## ## ─ Packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.43 2025-04-15 [1] RSPM (R 4.5.0) ## bslib 0.9.0 2025-01-30 [2] RSPM (R 4.5.0) ## cachem 1.1.0 2024-05-16 [2] RSPM (R 4.5.0) ## cli 3.6.4 2025-02-13 [2] RSPM (R 4.5.0) ## digest 0.6.37 2024-08-19 [2] RSPM (R 4.5.0) ## evaluate 1.0.3 2025-01-10 [2] RSPM (R 4.5.0) ## fastmap 1.2.0 2024-05-15 [2] RSPM (R 4.5.0) ## htmltools 0.5.8.1 2024-04-04 [2] RSPM (R 4.5.0) ## jquerylib 0.1.4 2021-04-26 [2] RSPM (R 4.5.0) ## jsonlite 2.0.0 2025-03-27 [2] RSPM (R 4.5.0) ## knitr 1.50 2025-03-16 [2] RSPM (R 4.5.0) ## lifecycle 1.0.4 2023-11-07 [2] RSPM (R 4.5.0) ## R6 2.6.1 2025-02-15 [2] RSPM (R 4.5.0) ## rlang 1.1.6 2025-04-11 [2] RSPM (R 4.5.0) ## rmarkdown 2.29 2024-11-04 [2] RSPM (R 4.5.0) ## rstudioapi 0.17.1 2024-10-22 [2] RSPM (R 4.5.0) ## sass 0.4.10 2025-04-11 [2] RSPM (R 4.5.0) ## sessioninfo * 1.2.3 2025-02-05 [2] RSPM (R 4.5.0) ## xfun 0.52 2025-04-02 [2] RSPM (R 4.5.0) ## yaml 2.3.10 2024-07-26 [2] RSPM (R 4.5.0) ## ## [1] /usr/local/lib/R/host-site-library ## [2] /usr/local/lib/R/site-library ## [3] /usr/local/lib/R/library ## * ── Packages attached to the search path. ## ## ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── This book was last updated on 2025-04-23 19:29:59.569791. "],["quick-start.html", "1 Quick Start 1.1 Setup 1.2 Configuration", " 1 Quick Start A brief guide to setting up BiocMAP. A more detailed and thorough guide is here. For troubleshooting advice, see our guide. 1.1 Setup Clone the BiocMAP repository with git clone git@github.com:LieberInstitute/BiocMAP.git Change directory into the repository with cd BiocMAP Run the command bash install_software.sh [option] to set up test files and software dependencies, where [option] must be replaced by docker, singularity, conda, local, or jhpce. These respectively (and in descending order of our recommendation) correspond to using docker to manage software dependencies, using singularity, using conda, installing all software locally, or preparing BiocMAP for execution at the JHPCE cluster. If using BiocMAP on a cluster, set the arioc_queue Arioc setting in your config file for the first module. Note: JHPCE users must also make an edit to their ~/.bashrc files, described here. 1.2 Configuration 1.2.1 Your “main” script The script you will use to run the pipeline depends on the system (“executor”) you wish to run the pipeline on, as well as which module you wish to run. Executor Module Script SGE cluster first run_first_half_sge.sh SGE cluster second run_second_half_sge.sh SLURM cluster first run_first_half_slurm.sh SLURM cluster second run_second_half_slurm.sh local machine first run_first_half_local.sh local machine second run_second_half_local.sh The JHPCE cluster first run_first_half_jhpce.sh The JHPCE cluster second run_second_half_jhpce.sh Options included in the main script should be modified as appropriate for the experiment. On SLURM and SGE clusters (including JHPCE), the main script should be submitted as a job (i.e. using sbatch or qsub). On local machines, the pipeline can be run interactively (i.e. bash run_pipeline_local.sh). 1.2.2 Your config file Your combination of “executor” (SLURM cluster, SGE cluster, or local) and “module” (first half or second half) determine the name of your configuration file. Find your file under BiocMAP/conf/. Executor Module Config Filename SGE cluster first first_half_sge.config SGE cluster second second_half_sge.config SLURM cluster first first_half_slurm.config SLURM cluster second second_half_slurm.config local machine first first_half_local.config local machine second second_half_local.config The JHPCE cluster first first_half_jhpce.config The JHPCE cluster second second_half_jhpce.config As an example, suppose you have access to a SLURM cluster, and wish to run the second module to process existing alignments. Your config file is then BiocMAP/conf/second_half_slurm.config. "],["setup-details.html", "2 Setup Details 2.1 Requirements 2.2 Installation 2.3 Run the Pipeline 2.4 Sharing the pipeline with many users", " 2 Setup Details 2.1 Requirements BiocMAP is designed for execution on Linux, and requires the following: Java 8 or later Access to NVIDIA GPU(s) during installation, locally or via a computing cluster, with recent stable NVIDIA video drivers and CUDA runtime. BiocMAP may also be installed without these, but only the second module will function in this case (recommended) docker, singularity, or Anaconda/ Miniconda If installing the pipeline locally (see installation), the following are also required: Python 3 (tested with 3.7.3), with pip R (tested with R 3.6-4.3) GNU make If installing the pipeline for use with docker (see installation), the NVIDIA container toolkit is also required for use of the first module. A CUDA runtime &gt;= 10.1 is required for docker/singularity users. Additionally, installation via the “local” or “conda” modes (see below) require a C compiler, such as GCC. An up-to-date version of gcc is often required to ensure R packages get properly installed. 2.2 Installation BiocMAP makes use of a number of different additional software tools. The user is provided four installation “modes” to automatically manage these dependencies: “docker”, “singularity”, “conda”, or “local” (which we recommend in descending order). Docker: The recommended option is to manage software with docker, if it is available. From within the repository, perform the one-time setup by running bash install_software.sh \"docker\". This installs nextflow and sets up some test files. When running BiocMAP, the required docker images are automatically pulled if not already present, and components of the pipeline run within the associated containers. A full list of the images that are used is here. If root permissions are needed to run docker, one can instruct the installation script to use sudo in front of any docker commands by running bash install_software.sh \"docker\" \"sudo\". Finally, if using BiocMAP on a cluster, set the arioc_queue Arioc setting in your config file for the first module. Singularity: If singularity is available, a user may run bash install_software.sh \"singularity\" to install BiocMAP. This installs nextflow and sets up some test files. When running BiocMAP, the required docker images are automatically pulled if not already present, and components of the pipeline run within the associated containers using singularity. A full list of the images that are used is here. Next, if using BiocMAP on a cluster, set the arioc_queue Arioc setting in your config file for the first module. Conda: If conda is available (through Anaconda or Miniconda), a user can run bash install_software.sh \"conda\" to fully install BiocMAP. This creates a conda environment within which the required software is locally installed, and sets up some test files. Note that this is a one-time procedure even on a shared machine (new users automatically make use of the installed conda environment). Finally, if using BiocMAP on a cluster, set the arioc_queue Arioc setting in your config file for the first module. Local install: Installation is performed by running bash install_software.sh \"local\" from within the repository. This installs nextflow, several bioinformatics tools, R and packages, and sets up some test files. A full list of software used is here. The script install_software.sh builds each software tool from source, and hence relies on some common utilities which are often pre-installed in many unix-like systems: A C/C++ compiler, such as GCC or Clang The GNU make utility The makeinfo utility git, for downloading some software from their GitHub repositories The unzip utility Please note that this installation method is experimental, and can be more error-prone than installation via the “docker”, “singularity”, or “conda” modes. Finally, if using BiocMAP on a cluster, set the arioc_queue Arioc setting in your config file for the first module. Note: users at the JHPCE cluster do not need to worry about managing software via the above methods (required software is automatically available through modules). Simply run bash install_software.sh \"jhpce\" to install any missing R packages and set up some test files. 2.2.1 Troubleshooting Some users may encounter errors during the installation process, particularly when installing software locally. We provide a list below of the most common installation-related issues. BiocMAP has been tested on: CentOS 7 (Linux) Ubuntu 18.04 (Linux) 2.2.1.1 CUDA runtime is not installed BiocMAP aligns samples to a reference genome using Arioc, a GPU-based software built with CUDA– we require that the CUDA toolkit is installed. During installation, if you encounter an error message like this: g++ -std=c++14 -c -Wall -Wno-unknown-pragmas -O3 -m64 -I /include -o CudaCommon/ThrustSpecializations.o CudaCommon/ThrustSpecializations.cpp In file included from CudaCommon/ThrustSpecializations.cpp:11:0: CudaCommon/stdafx.h:26:60: fatal error: cuda_runtime.h: No such file or directory #include &lt;cuda_runtime.h&gt; // CUDA runtime API ^ compilation terminated. it’s possible that the CUDA toolkit is not installed (and should be). On a computing cluster, it’s also possible CUDA-related software must be loaded, or is only available on a particular queue (associated with GPU resources). In the former case, check documentation or contact tech support to see if there is a proper way to load the CUDA toolkit for your cluster. For example, if your cluster uses Lmod environment modules, there might be a command like module load cuda that should be run before the BiocMAP installation script. If this works, you’ll need to adjust your configuration file for the first module following the advice here. In the latter case, try running the installation script from the queue containing the GPU(s). 2.2.1.2 g++ compilation errors For “conda” or “local” installation methods, g++ is used to compile Arioc. If your gcc version is too old, you may encounter errors during the installation process, likely during the step that compiles Arioc. Here is an example of an error message that could occur: g++: error: unrecognized command line option &#39;-std=c++14&#39; On a local machine, consider installing a newer gcc and g++ (though please note that versions later than 7 cannot compile Arioc!). On a cluster, similar to the advice here, contact tech support or your cluster’s documentation to see if there is a way to load particular versions of gcc. If your cluster uses Lmod environment modules, there might be a command like module load gcc that should be run before the BiocMAP installation script. We have successfully used gcc 5.5.0. If this works, you’ll need to adjust your configuration file for the first module following the advice here. 2.2.1.3 Using Lmod modules with Arioc For users encountering specific issues during installation via the “conda” or “local” modes on a computing cluster (in particular, see g++ compilation errors and cuda-related errors), loading an Lmod environment module before installation with install_software.sh might provide a solution if this is an option. After successful installation with install_software.sh while using a module, it will also be necessary to instruct BiocMAP to load this module whenever it performs alignment-related steps. To do this, locate your configuration file for the first module. As an example, if you needed a module called ‘cuda/10.0’ to perform installation, you can add the line module = 'cuda/10.0' in the processes EncodeReference, EncodeReads, and AlignReads. Here is what the modified configuration would look like for the EncodeReference process for SLURM users: withName: EncodeReference { cpus = 1 memory = 80.GB queue = params.arioc_queue module = &#39;cuda/10.0&#39; } To load two modules, such as gcc/5.5.0 and cuda/10.0, the syntax looks like: module = 'gcc/5.5.0:cuda/10.0'. 2.2.1.4 Singularity Issues When installing BiocMAP with Singularity (i.e. bash install_software.sh singularity), quite a bit of memory is sometimes required to build the Singularity images from their Docker counterparts, hosted on Docker Hub. Memory-related error messages can widely vary, but an example looks like this: INFO: Creating SIF file... FATAL: While making image from oci registry: while building SIF from layers: While running mksquashfs: exit status 1: FATAL ERROR:Failed to create thread Requesting more memory and reinstalling will solve such issues. 2.2.1.5 Java Issues With any installation method, the process may fail if Java is not installed or is sufficiently outdated (e.g. &lt; 11). In this case, installing a recent version of Java (Nextflow recommends between 11 and 18) will solve the issue. Here are some potential pieces of BiocMAP error messages that suggest Java is too outdated or improperly installed: NOTE: Nextflow is not tested with Java 1.8.0_262 -- It&#39;s recommended the use of version 11 up to 18 Error: A JNI error has occurred, please check your installation and try again Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: org/eclipse/jgit/api/errors/GitAPIException has been compiled by a more recent version of the Java Runtime (class file version 55.0), this version of the Java Runtime only recognizes class file versions up to 52.0 2.3 Run the Pipeline The “main” script used to run the pipeline depends on the environment you will run it on. 2.3.1 Run in a SLURM environment/ cluster (Optional) Adjust configuration: hardware resource usage, software versioning, and cluster option choices are specified in conf/first_half_slurm.config and conf/second_half_slurm.config. Modify the main script and run: the main scripts are run_first_half_slurm.sh and run_second_half_slurm.sh. Each script may be submitted to the cluster as a job (e.g. sbatch run_first_half_slurm.sh). See the full list of command-line options for other details about modifying the script for your use-case. To run the complete workflow, it is recommended to first submit run_first_half_slurm.sh, then monitor the output log run_first_half_slurm.log so that run_second_half_slurm.sh may be submitted when the log indicates the first half has completed. See here for Nextflow’s documentation regarding SLURM environments. 2.3.2 Run on a Sun Grid Engines (SGE) cluster (Optional) Adjust configuration: hardware resource usage, software versioning, and cluster option choices are specified in conf/first_half_sge.config and conf/second_half_sge.config. Modify the main script and run: the main scripts are run_first_half_sge.sh and run_second_half_sge.sh. Each script may be submitted to the cluster as a job (e.g. qsub run_first_half_sge.sh). See the full list of command-line options for other details about modifying the script for your use-case. To run the complete workflow, it is recommended to first submit run_first_half_sge.sh, then monitor the output log run_first_half_sge.log so that run_second_half_sge.sh may be submitted when the log indicates the first half has completed. See here for additional information on nextflow for SGE environments. 2.3.3 Run locally (Optional) Adjust configuration: hardware resource usage and other configurables are located in conf/first_half_local.config and conf/second_half_local.config. Modify the main script and run: the main scripts are run_first_half_local.sh and run_second_half_local.sh. After configuring options for your use-case (See the full list of command-line options), each script may be run interactively (e.g. bash run_first_half_local.sh). 2.3.4 Run on the JHPCE cluster (Optional) Adjust configuration: default configuration with thoroughly testing hardware resource specification is described within conf/first_half_jhpce.config and conf/second_half_jhpce.config. Modify the main script and run: the “main” scripts are run_first_half_jhpce.sh and run_second_half_jhpce.sh. Each script may be submitted to the cluster as a job (e.g. sbatch run_first_half_jhpce.sh). See the full list of command-line options for other details about modifying the script for your use-case. To run the complete workflow, it is recommended to first submit run_first_half_jhpce.sh, then monitor the output log run_first_half_jhpce.log so that run_second_half_jhpce.sh may be submitted when the log indicates the first half has completed. 2.3.5 Example main script Below is a full example of a typical main script for the first module, modified from the run_first_half_jhpce.sh script. At the top are some cluster-specific options, recognized by SGE, the grid scheduler at the JHPCE cluster. These are optional, and you may consider adding appropriate options similarly, if you plan to use BiocMAP on a computing cluster. After the main command, nextflow $ORIG_DIR/first_half.nf, each command option can be described line by line: --sample \"paired\": input samples are paired-end --reference \"hg38\": these are human samples, to be aligned to the hg38 reference genome --input \"/users/neagles/wgbs_test\": /users/neagles/wgbs_test is a directory that contains the samples.manifest file, describing the samples. --output \"/users/neagles/wgbs_test/out\": /users/neagles/wgbs_test/out is the directory (which possibly exists already) where pipeline outputs should be placed. -profile jhpce: configuration of hardware resource usage, and more detailed pipeline settings, is described at conf/jhpce.config, since this is a run using the JHPCE cluster -w \"/fastscratch/myscratch/neagles/nextflow_work\": this is a nextflow-specific command option (note the single dash), telling BiocMAP that temporary files for the pipeline run can be placed under /fastscratch/myscratch/neagles/nextflow_work. --trim_mode \"force\": this optional argument instructs BiocMAP to trim all samples. Note there are alternative options. -profile first_half_jhpce: this line, which should typically always be included, tells BiocMAP to use conf/first_half_jhpce.config as the configuration file applicable to this pipeline run. #!/bin/bash #SBATCH -q shared #SBATCH --mem=25G #SBATCH --job-name=BiocMAP #SBATCH -o ./run_first_half_jhpce.log #SBATCH -e ./run_first_half_jhpce.log # After running &#39;install_software.sh&#39;, this should point to the directory # where BiocMAP was installed, and not say &quot;$PWD&quot; ORIG_DIR=$PWD module load nextflow/20.01.0 export _JAVA_OPTIONS=&quot;-Xms8g -Xmx10g&quot; nextflow $ORIG_DIR/first_half.nf \\ --sample &quot;paired&quot; \\ --reference &quot;hg38&quot; \\ --input &quot;/users/neagles/wgbs_test&quot; \\ --output &quot;/users/neagles/wgbs_test/out&quot; \\ -w &quot;/fastscratch/myscratch/neagles/nextflow_work&quot; \\ --trim_mode &quot;force&quot; \\ -profile first_half_jhpce 2.4 Sharing the pipeline with many users A single installation of BiocMAP can be shared among potentially many users. New users can simply copy the appropriate “main” script (determined above) to a different desired directory, and modify the contents as appropriate for the particular experiment. Similarly, a single user can copy the “main” script and modify the copy whenever there is a new experiment/ set of samples to process, reusing a single installation of BiocMAP arbitrarily many times. Note It is recommended to use a unique working directory with the -w option for each experiment. This ensures: BiocMAP resumes from the correct point, if ever stopped while multiple users are running the pipeline Deleting the work directory (which can take a large amount of disk space) does not affect BiocMAP execution for other users or other experiments 2.4.1 Customizing execution for each user By default, all users will share the same configuration. This likely suffices for many use cases, but alternatively new configuration files can be created. Below we will walk through an example where a new user of a SLURM-based cluster wishes to use an existing BiocMAP installation to run the first module, but wants a personal configuration file to specify different annotation settings. Copy the existing configuration to a new file # Verify we are in the BiocMAP repository pwd # Create the new configuration file cp conf/first_half_slurm.config conf/my_first_half_slurm.config Modify the new file as desired Below we will change the GENCODE release to the older release 25, for human, via the gencode_version_human variable. params { //---------------------------------------------------- // Annotation-related settings //---------------------------------------------------- gencode_version_human = &quot;25&quot; // originally was &quot;34&quot;! gencode_version_mouse = &quot;M23&quot; anno_build = &quot;main&quot; // main or primary (main is canonical seqs only) See configuration for details on customizing BiocMAP settings. Add the new file as a “profile” This involves adding some code to nextflow.config, as shown below. profiles { // Here we&#39;ve named the new profile &quot;my_new_config&quot;, and pointed it to the // file &quot;conf/my_first_half_slurm.config&quot;. my_new_config { includeConfig &#39;conf/my_first_half_slurm.config&#39; } Reference the new profile in the “main” script Recall that new users should copy the “main” script and modify the copy as appropriate. In this case, we open a copy of the original run_first_half_slurm.sh: # At the nextflow command, we change the &#39;-profile&#39; argument at the bottom $ORIG_DIR/Software/bin/nextflow $ORIG_DIR/first_half.nf \\ --sample &quot;paired&quot; \\ --reference &quot;hg38&quot; \\ -profile my_new_config # this was changed from &quot;-profile first_half_slurm&quot;! "],["command-opts.html", "3 All Command Options 3.1 BiocMAP Options 3.2 Nextflow Options", " 3 All Command Options 3.1 BiocMAP Options Each of the below parameters, unless otherwise noted, applies to both the first and second module provided in BiocMAP. 3.1.1 Mandatory Parameters --sample “single” or “paired”: whether reads are single-end or paired-end --reference “hg38”, “hg19”, or “mm10”. The reference genome to be used for alignment and methylation extraction 3.1.2 Optional Parameters --all_alignments (First module only) Include this flag to signal Arioc to also write outputs for discondant, rejected, and unmapped reads. Sam files for each outcome are kept as pipeline outputs. In either case, only concordant reads are used for later processing (methylation extraction and beyond) --annotation The path to the directory containing reference-related files. Defaults to “./ref” (relative to the repository). If annotation files are not found here, the pipeline includes a step to build them. --custom_anno [label] Include this flag to indicate that the directory specified with --annotation [dir] includes user-provided annotation files to use instead of the default files. See the “Using custom annotation” section for more details. --input If using the first module, the path to the directory containing the “samples.manifest” file, or, if using the second module, the path to the directory containing the “rules.txt” file. Defaults to “./test” (relative to the repository) --output The path to the directory to store pipeline output files. Defaults to “./results” (relative to the repository) --trim_mode (First module only) Determines the conditions under which trimming occurs: “skip”: do not perform trimming on samples “adaptive”: [default] perform trimming on samples that have failed the FastQC “Adapter content” metric “force”: perform trimming on all samples --use_bme (Second module only) Include this flag to perform methylation-extraction-related processes with Bismark utilities, rather than the default of MethylDackel --with_lambda (Second module only) Include this flag if all samples have spike-ins with the lambda bacteriophage genome. Pseudoalignment will then be performed to estimate bisulfite conversion efficiency 3.2 Nextflow Options The nextflow command itself provides many additional options you may add to your “main” script. A few of the most commonly applicable ones are documented below. For a full list, type [path to nextflow] run -h- the full list does not appear to be documented at nextflow’s website. -w [path] Path to the directory where nextflow will place temporary files. This directory can fill up very quickly, especially for large experiments, and so it can be useful to set this to a scratch directory or filesystem with plenty of storage capacity. -resume Include this flag if pipeline execution halts with an error for any reason, and you wish to continue where you left off from last run. Otherwise, by default, nextflow will restart execution from the beginning. -with-report [filename] Include this to produce an html report with execution details (such as memory usage, completion details, and much more) N [email address] Sends email to the specified address to notify the user regarding pipeline completion. Note that nextflow relies on the sendmail tool for this functionality- therefore sendmail must be available for this option to work. "],["pipeline-overview.html", "4 Pipeline Overview 4.1 Preparation Steps 4.2 Main Workflow Steps: First Module 4.3 Main Workflow Steps: Second Module", " 4 Pipeline Overview Diagram representing the “conceptual” workflow traversed by BiocMAP. Here some nextflow processes are grouped together for simplicity; the exact processes traversed are enumerated below. The red box indicates the FASTQ files are inputs to the pipeline; green coloring denotes major output files from the pipeline; the remaining boxes represent computational steps. Yellow-colored steps are optional or not always performed; for example, preparing a particular set of annotation files occurs once and uses a cache for further runs. Finally, blue-colored steps are ordinary processes which occur on every pipeline execution. 4.1 Preparation Steps The following processes in the pipeline are done only once for a given configuration, and are skipped on all subsequent runs: 4.1.1 Downloading and preparing required annotation files PullReference: when using default annotation, this process pulls the genome fasta from GENCODE, possibly subsets to canonical sequences (see annotation builds, and saves to the directory specified by --annotation. This is the file against which FASTQ reads are aligned, and methylation is extracted. PrepareReference: when run as part of the first module: split the single FASTA file into individual files (for handling with Arioc) and write the configuration file later used by Arioc to encode these reference files. When run as part of the second module: runs bismark_genome_preparation with the --hisat option, in preparation for possible methylation extraction using Bismark (though note that MethylDackel is used instead by default!). EncodeReference: run AriocE to encode reference files for use with the Arioc aligner. This process requires a GPU. PrepareLambda when using the --with_lambda option as part of the second module, this process downloads the lambda bacteriophage genome and creates kallisto indices required for later estimation of bisulfite conversion efficiency. 4.2 Main Workflow Steps: First Module PreprocessInputs: Merge FASTQ files where specified in samples.manifest and create conveniently named soft links of each FASTQ file for internal use in BiocMAP. FastQC_Untrimmed: FASTQ files are ran through FastQC as a preliminary quality control measure. By default, the presence of “adapter content” determined in this process decides whether a given sample is trimmed in the next step. See the --trim_mode command option for modifying this default behavior. Trimming: See “FastQC_Untrimmed” above- FASTQ inputs are trimmed with Trim Galore! either based on adapter content from FastQC, or differently based on the --trim_mode [command option]. FastQC is also run as a confirmation step after trimming. For samples that are trimmed, post-trimming FastQC metrics are collected and later gathered into an R data frame; for all other samples, these metrics are collected from the FastQC run prior to any trimming. WriteAriocConfigs: Generate configurations file for use in AriocE, followed by AriocU or AriocP, using configuration settings specified in the appropriate config. Note two configuration files are created for each sample. EncodeReads: Run AriocE to encode each sample in preparation for the alignment step with Arioc. AlignReads: Align each sample to the reference genome using Arioc, creating at least one SAM file. FilterAlignments: Filter, sort, compress, and index alignments from Arioc. First, primary alignments with a MAPQ of at least 5 are kept, and duplicate alignments are removed using samblaster. The result is coordinate-sorted, compressed into BAM format, and this BAM file is indexed. MakeRules: A rules.txt file is created in the same directory as samples.manifest so that the second module can be easily run by the user after completion of the first module. 4.3 Main Workflow Steps: Second Module PreprocessInputs: parse rules.txt and samples.manifest, the inputs to the second module, so that all relevant FASTQ files and logs may be handled correctly in later steps. LambdaPseudo: when the --with_lambda option is specified, pseudoalignment to the lambda bacteriophage transcriptome is performed. Both the (overwhelmingly unmethylated) original transcriptome and then an “in-silico bisulfite-converted” version of the transcriptome are used in series. Successful map rates are used to estimate the entire experiment’s bisulfite conversion rate, a metric which is included in the final output R data frame. BME: when using the --use_bme option, methylation extraction is performed using bismark_methylation_extraction. Bismark2Bedgraph: when using the --use_bme option, the bismark2bedgraph utility from Bismark is run to generate intermediate bedGraph files, which are later used to generate text-based “cytosine reports”. Coverage2Cytosine: when using the --use_bme option, the coverage2cytosine utility from Bismark created text-based “cytosine reports”, which are later read into R to produce the final bsseq objects. MethylationExtraction: this process is run by default in place of the above Bismark-related processes, and performs methylation extraction with MethylDackel, ultimately generating “cytosine reports” comparable to those produced by the coverage2cytosine Bismark utility. ParseReports: logs from FastQC, any trimming, alignment, methylation extraction, and, if applicable, bisulfite conversion estimation, are parsed to extract key metrics into an R data frame. This data frame is produced as both a standalone .rda file, and included as part of the colData slots in each of the output bsseq objects. FormBsseqObjects: bsseq R objects are formed, one for each canonical chromosome and all samples, from information provided in the “cytosine reports” generated in either the MethylationExtraction or Coverage2Cytosine process. These are typically considered intermediary files, as the next process merges the many bsseq objects from this step into just a pair of final outputs, but for very large experiments, the objects from this step may be more reasonable to work with (to reduce memory requirements). MergeBsseqObjects: the final process in the pipeline, which combines all previous bsseq objects into just two objects- the first containing all cytosines in “CpG” context in the genome, and the other containing those in “CpH” context. The colData slot in each of these objects contain the metrics extracted in the ParseReports process, some of which might be useful covariates in potential downstream statistical analyses. See outputs for more information. "],["annotation.html", "5 Annotation 5.1 Default Annotation 5.2 Custom Annotation", " 5 Annotation BiocMAP can be run with hg38, hg19, or mm10 references. The pipeline has a default and automated process for pulling and building annotation-related files, but the user can opt to provide their own annotation as an alternative. Both of these options are documented below. Example annotation files below are the ones used with default configuration when hg38 reference is selected. A genome assembly fasta: the reference genome to align reads to, like the file here (but unzipped). Gene annotation gtf: containing transcript data, like the file here (but unzipped). The lambda transcriptome: for experiments utilizing spike-ins of the lambda bacteriophage genome, the transcriptome provided here is used (but unzipped). 5.1 Default Annotation BiocMAP uses annotation files provided by GENCODE. 5.1.1 Choosing a release With genome annotation constantly being updated, the user may want to use a particular GENCODE release. The configuration variables gencode_version_human and gencode_version_mouse control which GENCODE release is used for the human and mouse genomes, respectively. params { //---------------------------------------------------- // Annotation-related settings //---------------------------------------------------- gencode_version_human = &quot;34&quot; gencode_version_mouse = &quot;M23&quot; anno_build = &quot;main&quot; // main or primary (main is canonical seqs only) 5.1.2 Choosing a “build” Depending on the analysis you are doing, you may wish to only consider the reference chromosomes (for humans, the 25 sequences “chr1” through “chrM”) for alignment and methylation extraction. BiocMAP provides the option to choose from two annotation “builds” for a given release and reference, called “main” and “primary” (following the naming convention from GENCODE databases). The “main” build consists of only the canonical “reference” sequences for each species The “primary” build consists of the canonical “reference” sequences and additional scaffolds, as a genome primary assembly fasta from GENCODE would contain. See the variable annotation_build in your configuration file for making this selection for your pipeline run. 5.2 Custom Annotation You may wish to provide a genome FASTA (the reference genome to align reads to), such as the file here, in place of the automatically managed GENCODE files described in the above section. You must also add the --custom_anno [label] argument to your execution scripts, to specify you are using custom annotation files. The “label” is a string you want to include in filenames generated from the annotation files you provided. This is intended to allow the use of potentially many different custom annotations, assigned a unique and informative name you choose each time. This can be anything except an empty string (which internally signifies not to use custom annotation). "],["inputs.html", "6 Inputs 6.1 The samples.manifest File 6.2 The rules.txt File", " 6 Inputs The only input to the first module is a file called samples.manifest. This user-created file associates each FASTQ file with a path and ID, and allows the pipeline to automatically merge files if necessary. After creating the samples.manifest file, one must add the --input argument to the approriate execution script for the first module, and its value should be the path to the directory containing samples.manifest. For example, if the full path is /users/neagles/samples.manifest, the full argument in the execution script should be --input \"/users/neagles\". The second module requires the same samples.manifest file and an additional file called rules.txt. The latter specifies where relevant logs and the samples.manifest file are located. Similarly as in the first module, one must add the --input argument to the approriate execution script for the second module, but its value should be the path to the directory containing rules.txt. Please note that running the first module automatically creates a rules.txt file in the same directory as the input samples.manifest, so one can use the same --input argument for both the first and second module when running them in series. 6.1 The samples.manifest File 6.1.1 What samples.manifest should look like Each line in samples.manifest should have the following format: For a set of unpaired reads &lt;PATH TO FASTQ FILE&gt;(tab)&lt;optional MD5&gt;(tab)&lt;sample label/id&gt; For paired-end sets of reads &lt;PATH TO FASTQ 1&gt;(tab)&lt;optional MD5 1&gt;(tab)&lt;PATH TO FASTQ 2&gt;(tab)&lt;optional MD5 2&gt;(tab)&lt;sample label/id&gt; A line of paired-end reads could look like this: WGBS_sample1_read1.fastq 0 WGBS_sample1_read2.fastq 0 sample1 The MD5(s) on each line are for compatibility with a conventional samples.manifest structure, and are not explicitly checked in the pipeline (you may simply use 0s as in the above example). Paths must be long/full. If you have a single sample split across multiple files, you can signal for the pipeline to merge these files by repeating the sample label/id on each line of files to merge. A samples.manifest file cannot include both single-end and paired-end reads; separate pipeline runs should be performed for each of these read types. This is an example of a samples.manifest file for some paired-end samples. Note how the first sample “dm3” is split across more than one pair of files, and is to be merged: /scratch/dm3_file1_1.fastq 0 /scratch/dm3_file1_2.fastq 0 dm3 /scratch/dm3_file2_1.fastq 0 /scratch/dm3_file2_2.fastq 0 dm3 /scratch/sample_01_1.fastq.gz 0 /scratch/sample_01_2.fastq.gz 0 sample_01 /scratch/sample_02_1.fastq.gz 0 /scratch/sample_02_2.fastq.gz 0 sample_02 6.1.2 More details regarding inputs Input FASTQ files can have the following file extensions: .fastq, .fq, .fastq.gz, .fq.gz. All FASTQ files associated with the same sample ID must use the same extenstion. FASTQ files must not contain “.” characters before the typical extension (e.g. sample.1.fastq), since some internal functions rely on splitting file names by “.”. 6.1.3 Creating a manifest file In a common scenario, you may have a large number of FASTQ files in a single directory, for a given experiment. How can the samples.manifest file be constructed in this case? While the method you use is a matter of preference, we find it straightforward to write a small R script to generate the manifest. Suppose we have 3 paired-end samples, consisting of a total of 6 FASTQ files: /data/fastq/SAMPLE1_L001_R1_001.fastq.gz /data/fastq/SAMPLE1_L001_R2_001.fastq.gz /data/fastq/SAMPLE2_L002_R1_001.fastq.gz /data/fastq/SAMPLE2_L002_R2_001.fastq.gz /data/fastq/SAMPLE3_L003_R1_001.fastq.gz /data/fastq/SAMPLE3_L003_R2_001.fastq.gz The following script can generate the manifest appropriate for this experiment: # If needed, install the &#39;jaffelab&#39; GitHub-based package, which includes a # useful function for string manipulation remotes::install_github(&quot;LieberInstitute/jaffelab&quot;) library(&quot;jaffelab&quot;) fastq_dir &lt;- &quot;/data/fastq&quot; # We can take advantage of the uniform file naming convention to get the paths # of each mate in the pair, for every sample. Here we use a somewhat # complicated regular expression to match file names (to be sure we are # matching precisely the files we think we&#39;re matching), but this can be kept # simple if preferred. r1 &lt;- list.files(fastq_dir, &quot;.*_L00._R1_001\\\\.fastq\\\\.gz&quot;, full.names = TRUE) r2 &lt;- list.files(fastq_dir, &quot;.*_L00._R2_001\\\\.fastq\\\\.gz&quot;, full.names = TRUE) # We can form a unique ID for each sample by taking the portion of the path to # the first read preceding the lane and mate identifiers. The function &#39;ss&#39; is # a vectorized form of &#39;strsplit&#39;, handy for this task ids &lt;- ss(basename(r1), &quot;_L00&quot;) # Sanity check: there should be the same number of first reads as second reads stopifnot(length(R1) == length(R2)) # Prepare the existing sample information into the expected format (for now, # as a character vector where each element will be a line in # &#39;samples.manifest&#39;). We will simply use zeros for the optional MD5 sums. manifest &lt;- paste(r1, 0, r2, 0, ids, sep = &quot;\\t&quot;) # Write the manifest to a file (in this case, in the current working # directory) writeLines(manifest, con = &quot;samples.manifest&quot;) 6.2 The rules.txt File This input file to the second module is automatically produced when running the first module, and placed in the same directory as the input samples.manifest. The below documentation exists primarily to describe how to manually produce rules.txt if the first and second modules are run on different systems, or if the steps corresponding to the first module are performed outside of BiocMAP. 6.2.1 What rules.txt should look like # An example &#39;rules.txt&#39; file manifest = /users/nick/samples.manifest sam = /users/nick/Arioc/[id]/sams/[id].cfu.sam arioc_log = /users/nick/Arioc/[id]/log/AriocP.[id].log xmc_log = /users/nick/Arioc/[id]/log/[id].cfu.XMC.log trim_report = /users/nick/trim_galore/[id]/[id].fastq.gz_trimming_report.txt fastqc_log_last = /users/nick/FastQC/[id]_trimmed_summary.txt fastqc_log_first = /users/nick/FastQC/[id]_untrimmed_summary.txt This file consists of several lines of key-value pairs, where keys and values are separated by an equals sign (“=”) and optionally spaces. Lines without “=” are ignored, and can be used as comments. The above example starts such lines with “#” for clarity. The required keys to include in a valid rules.txt file include “manifest”, “sam”, “arioc_log”, and “trim_report”. The associated values for these keys are the paths to the samples.manifest file, the filtered/deduplicated alignment SAMs, verbose output logs from Arioc alignment, and output logs from TrimGalore!, respectively. Note that users who plan to use MethylDackel for methylation extraction (the default!) should have coordinate-sorted and indexed BAM files ready, which are passed to the “sam” key using a glob expression. Otherwise, just one SAM file is expected per sample. Note this example for the former case: # Suppose we have 4 files across 2 samples: # /users/nick/alignments/id1_sorted.bam and /users/nick/alignments/id1_sorted.bam.bai # /users/nick/alignments/id2_sorted.bam and /users/nick/alignments/id2_sorted.bam.bai # We can point to these files using the &#39;sam&#39; key in &#39;rules.txt&#39;: sam = /users/nick/alignments/[id]_sorted.bam* Optional keys accepted in a rules.txt file include “xmc_log”, “fastqc_log_first”, and “fastqc_log_last”. Associated values are paths to logs from XMC (a utility that comes with the Arioc software), *summary.txt logs from FastQC, and logs from the latest or only run of FastQC (see the below point about optional keys), respectively. The optional “fastqc_log_first” key can be used when the user runs FastQC twice (for example, before and after trimming) for at least one sample. In this case, “fastqc_log_first” refers to the pre-trimming run of FastQC. Typically, methylation extraction is done in the second module, and the “xmc_log” option is only included for compatibility with a previous WGBS processing approach. Information from these logs will be included in the R data frame output from the second module, if these logs are present and specified in rules.txt. Since exact paths are different between samples, including “[id]” in the value field in a line of rules.txt indicates that the path for any particular sample can be found by replacing “[id]” with its sample ID. Otherwise, paths are interpreted literally. For paired-end experiments, there will be two summary logs from FastQC. To correctly describe this in rules.txt, a glob expression may be used. As an example, suppose we have FastQC summary logs for a sample called sample1, given by the paths /some_dir/fastqc/sample1_1_summary.txt and /some_dir/fastqc/sample1_2_summary.txt. The following line in rules.txt would be appropriate: fastqc_log_last = /some_dir/fastqc/[id]_[12]_summary.txt Be careful with forming glob expressions! If we had a 10-sample experiment with sample names sample1 through sample10 but used a simpler asterisk “*” in place of the alternation “_[12]” in the above example, we would get an error! # This is wrong! When finding files for &quot;sample1&quot;, we&#39;d also match files for # &quot;sample10&quot;! fastqc_log_last = /some_dir/fastqc/[id]*_summary.txt The --input [dir] argument to the nextflow command in run_second_half_*.sh scripts specifies the directory containing the rules.txt file, and is a required argument. "],["outputs.html", "7 Outputs 7.1 Main Outputs 7.2 Intermediary Outputs", " 7 Outputs 7.1 Main Outputs Main Outputs The major outputs from the second module are R objects from the bsseq Bioconductor package, which contain methylation proportion and coverage information at all cytosine loci in the human genome. bsseq extends the SummarizedExperiment class, which provides a general and popular format for storing genomics data. Two bsseq objects are produced, with one object containing cytosine sites in CpG context, and the other containing the remaining CpH loci. 7.1.1 CpG bsseq object We “strand collapse” CpG loci, which involves combining methylation data from both genomic strands (and thus discarding strand-specific information). The object is “smoothed” with the BSmooth algorithm, a process for inferring stable regional estimates of methylation levels. Loci are ordered by genomic position. 7.1.2 CpH bsseq object We retain strand-specific information for CpH loci. These loci are also ordered by genomic position. 7.1.3 Storage method Because all cytosines in the genome are included in the output objects, the data may occupy tens or even hundreds of gigabytes in memory (RAM) if loaded in a typical fashion. To enable working with the objects in a reasonable amount of memory, the assays (in this case methylation fraction and coverage counts) are HDF5-backed using the HDF5Array R package, based on the DelayedArray framework. Essentially, this involves storing the assays in a .h5 file, a format designed to enable working with on-disk data as if it were loaded in RAM. 7.1.4 Metrics The colData slot of each bsseq object includes metrics collected from various processing steps throughout the pipeline. These metrics are also included in a standalone RDA file containing an R data frame whose columns are different quality metrics, and whose rows are associated with sample names. A list of the exact column names and their descriptions is given below. Metric Name Processing step Description FQCbasicStats FastQC Value for “Basic Statistics” in FastQC summary output perBaseQual FastQC Value for “Per base sequence quality” in FastQC summary output perTileQual FastQC Value for “Per tile sequence quality” in FastQC summary output perSeqQual FastQC Value for “Per sequence quality scores” in FastQC summary output perBaseContent FastQC Value for “Per base sequence content” in FastQC summary output GCcontent FastQC Value for “Per sequence GC content” in FastQC summary output Ncontent FastQC Value for “Per base N content” in FastQC summary output SeqLengthDist FastQC Value for “Sequence Length Distribution” in FastQC summary output SeqDuplication FastQC Value for “Sequence Duplication Levels” in FastQC summary output OverrepSeqs FastQC Value for “Overrepresented sequences” in FastQC summary output AdapterContent FastQC Value for “Adapter Content” in FastQC summary output KmerContent FastQC Value for “Kmer Content” in FastQC summary output Total_reads_processed Trim Galore! Value for “Total reads processed” entry in STDOUT from trimming Reads_with_adapters Trim Galore! Value for “Reads with adapters” entry in STDOUT from trimming Reads_written_passing_filters Trim Galore! Value for “Reads written (passing filters)” entry in STDOUT from trimming Total_basepairs_processed Trim Galore! Value for “Total basepairs processed” entry in STDOUT from trimming Quality_trimmed Trim Galore! Value for “Quality-trimmed” entry in STDOUT from trimming Total_written_filtered Trim Galore! Value for “Total written (filtered)” entry in STDOUT from trimming Sequence Trim Galore! Detected adapter sequence when trimming was_trimmed Trim Galore! Logical value (“TRUE” or “FALSE”) indicating whether this sample was trimmed pairs Arioc (Paired-end only) Total number of pairs present in input data conc_pairs_total Arioc (Paired-end only) Total number of concordantly aligned pairs conc_pairs_1_mapping Arioc (Paired-end only) Number of uniquely mapped concordant alignments conc_pairs_many_mappings Arioc (Paired-end only) Number of non-uniquely mapped concordant pairs disc_pairs Arioc (Paired-end only) Number of pairs that aligned discordantly rejected_pairs Arioc (Paired-end only) Number of pairs that were “rejected” during alignment unmapped_pairs Arioc (Paired-end only) Number of pairs that didn’t both successfully map mates_not_in_paired_maps_total Arioc (Paired-end only) Value for “mates not in paired mappings” field in “SAM output” section of verbose output from alignment mates_NIPM_with_no_maps Arioc (Paired-end only) Number of mates not in paired mappings having no mappings mates_NIPM_with_1_map Arioc (Paired-end only) Number of mates not in paired mappings having 1 mapping mates_NIPM_with_many_maps Arioc (Paired-end only) Number of mates not in paired mappings having 2 or more mappings total_mapped_mates Arioc (Paired-end only) Total number of mates mapped maxQlen Arioc Value for “maximum Q length” field in “SAM output” section of verbose output from alignment max_diag_band_width Arioc (Paired-end only) Value for “maximum diagonal band width” field in “SAM output” section of verbose output from alignment TLEN_mean Arioc (Paired-end only) Mean fragment length TLEN_sd Arioc (Paired-end only) Standard deviation of fragment length TLEN_skewness Arioc (Paired-end only) Skewness of fragment length reads Arioc (Single-end only) Total number of reads present in input data mapped_reads_total Arioc (Single-end only) Number of reads successfully mapped to the reference genome mapped_reads_1_map Arioc (Single-end only) Number of reads mapped to a unique location on the reference genome mapped_reads_many_maps Arioc (Single-end only) Number of reads mapped to more than 1 location on the reference genome unmapped_reads Arioc (Single-end only) Number of reads that didn’t successfully map to the reference genome duplicate_maps Arioc Value for “duplicate mappings (unreported)” field in “SAM output” section of verbose output from alignment err_rate_primary_maps Arioc Observed error rate for reads (single-end) or pairs (paired-end) in primary mappings perc_M_CpG Methylation extraction Among all cytosines observed in CpG context, the percent that are methylated perc_M_CHG Methylation extraction Among all cytosines observed in CHG context, the percent that are methylated perc_M_CHH Methylation extraction Among all cytosines observed in CHH context, the percent that are methylated lambda_bs_conv_eff Lambda pseudo-alignment Estimated bisulfite conversion efficiency as a percentage 7.2 Intermediary Outputs BiocMAP generates a number of files along the process before producing the main outputs of interest in each module. Each of these “intermediary” files is described below. 7.2.1 First Module Preprocessing Logs preprocessing/ preprocess_input_first_half.log: Information about how the input samples.manifest file was parsed to internally handle input files correctly downstream. FastQC Outputs fastQC/ [trim_status]/[file_name]/*: Outputs from FastQC. Here [trim_status] indicates when FastQC was performed: Untrimmed is before trimming, and Trimmed is after. [file_name] contains the sample name, and if applicable, the mate number. Trimmed FASTQ Files trimming/ [sample_name]_trimmed*.fastq: Trimmed FASTQ files, if applicable, from Trim Galore. Raw Alignment-related Files Arioc/ sams/[sample_name].c.sam: Concordant alignments output directly from Arioc, in SAM format. configs/[sample_name]_align.cfg: The exact config file used by Arioc to align reads to the reference genome. configs/[sample_name]_encode.cfg: The exact config file used by Arioc to encode reads prior to alignment to the reference genome. logs/*_[sample_name].log: Logs from encoding and aligning samples with Arioc. Filtered Alignments FilteredAlignments/bams [sample_name].cfu.sorted.bam and [sample_name].cfu.sorted.bam.bai: Coordinate-sorted, quality-filtered (only including alignments with MAPQ &gt;= 5), unique (duplicate mappings removed) alignments in BAM format, with a corresponding BAM index. 7.2.2 Second Module Preprocessing Logs preprocessing/ preprocess_input_second_half.log: Information about how the input rules.txt file was parsed to internally handle input files correctly downstream. Lambda-based Bisulfite-Conversion Estimatation lambda/ [sample_name]_lambda_pseudo.log: (For experiments with lambda spike-ins, using the --with_lambda option) Logs containing a percentage estimate for the bisulfite-conversion efficiency for each sample, based on pseudo-aligning input FASTQs to both the lambda bacteriophage transcriptome and an “in silico bisulfite-converted copy” of the transcriptome, and comparing alignment rates. Bismark Methylation Extractor Outputs BME/ [sample_name]/*: (When using the --use_bme option) Files output from running bismark_methylation_extractor, including compressed text-based methylation information, a splitting report, and an “M-bias” report. Cytosine Reports and Bedgraphs Reports/ [sample_name]/[sample_name].*.CX_report.txt: “Cytosine reports” containing methylation counts and context for every cytosine in the genome, split by chromosome. By default, these are generated with MethylDackel, or otherwise with coverage2cytosine from the Bismark software suite when using the --use_bme option. [sample_name]/[sample_name]_bedgraph_merged*.gz: (When using the --use_bme option) A bedgraph file containing positional and methylation information for each cytosine; a “coverage” file containing similar information. See Bismark’s documentation for more detail. Individual bsseq objects BSobjects/objects/ [chromosome]/[context]/*: R objects containing either cytosines in “CpG” or “CpH”, for a single chromosome. While the final bsseq objects are combined across all chromosomes for convenience, it may be more practical in some cases to work with these individual objects, which are significantly smaller in memory. "],["configuration.html", "8 Configuration 8.1 Specifying Options for your Cluster 8.2 BiocMAP settings", " 8 Configuration BiocMAP is designed to be highly customizable, yet need no configuration from a user wishing to rely on sensible default settings. Most configuration, including software settings, hardware resources such as memory and CPU use, and more, can be done in a single file determined here. Please note that the common, “major” options are specified in your main script, and not in the configuration file. The config is intended to hold the “finer details”, which are documented below. While memory usage for most processes in BiocMAP is independent of the number of samples in the experiment, the FormBsseqObjects and MergeBsseqObjects processes in the second module scale roughly linearly with dataset size. In our experiments, 120GB of memory is sufficient for the MergeBsseqObjects process for 4 human samples, and 200GB is sufficient for as many as 600 human samples. As a rule of thumb, allow 1-2TB of disk space per human sample. BiocMAP execution time is nearly linear in the number of samples, taking roughly 1 day per 4 human samples (about 5 months for ~600 samples– while lengthy, an analysis of this scale is prohibitively long with CPU-based pipelines to the point of likely being infeasible). 8.1 Specifying Options for your Cluster In many cases, a user has access to a computing cluster which they intend to run BiocMAP on. If your cluster is SLURM or SGE-based, the pipeline is pre-configured with options you may be used to specifying (such as disk usage, time for a job to run, etc). However, these are straightforward to modify, should there be a need/desire. Common settings are described in detail below; however, a more comprehensive list of settings from nextflow can be found here. 8.1.1 Time The maximum allowed run time for a process, or step in the pipeline, can be specified. This may be necessary for users who are charged based on run time for their jobs. 8.1.1.1 Default for all processes The simplest change you may wish to make is to relax time constraints for all processes. The setting for this is here: executor { name = &#39;sge&#39; queueSize = 40 submitRateLimit = &#39;1 sec&#39; exitReadTimeout = &#39;40 min&#39; } process { cache = &#39;lenient&#39; time = 10.hour // this can be adjusted as needed While the syntax is not strict, some examples for properly specifying the option are time = '5m', time = '2h', and time = '1d' (minutes, hours, and days, respectively). 8.1.1.2 Specify per-process Time restrictions may also be specified for individual workflow steps. This can be done with the same syntax- suppose you wish to request 30 minutes for a given sample to be trimmed (note process names here). In the “process” section of your config, find the section labelled “withName: Trimming” as in the example here: withName: Trimming { cpus = 1 memory = 5.GB time = &#39;30m&#39; // specify a maximum run time of 30 minutes } 8.1.2 Cluster-specific options In some cases, you may find it simpler to directly specify options accepted by your cluster. For example, SGE users with a default limit on the maximum file size they may directly adjust the “h_fsize” variable as below: withName: FilterAlignments { cpus = 2 penv = &#39;local&#39; memory = 16.GB clusterOptions = &#39;-l h_fsize=800G&#39; } As with the time option, this can be specified per-process or for all processes. Any option recognized by your cluster may be used. 8.2 BiocMAP settings A number of variables in your config file exist to control choices about annotation to use, options passed to software tools, and more. These need not necessarily be changed, but allow more precise control over the pipeline if desired. Values for these variables may be changed in the “params” section of your config (near the top). A full descriptive list is provided below. 8.2.1 Annotation settings gencode_version_human: the GENCODE release to use for default (non-custom) annotation, when “hg38” or “hg19” references are used. A string, such as “32”. gencode_version_mouse: the GENCODE release to use for default (non-custom) annotation, when “mm10” reference is used. A string, such as “M23”. anno_build: controls which sequences are used in analysis with default (non-custom) annotation. “main” indicates only canonical reference sequences; “primary” includes additional scaffolds. 8.2.2 Arioc settings Below, we document how the names of settings accepted by BiocMAP correspond to variables in Arioc configuration. For documentation on those Arioc configuration variables, see here. arioc_queue: when running BiocMAP on an SGE/SLURM-based cluster, it is assumed GPU resources are available via a “queue” on the cluster. This variable must be set by the user to the queue name, in order to properly run BiocMAP. batch_size: this is passed to the “batchSize” attribute of the “AriocU” or “AriocP” element of the config for AriocU or AriocP. gapped_seed: this is passed to the “seed” attribute of the “gapped” element of the config for AriocU or AriocP. nongapped_seed: this is passed to the “seed” attribute of the “nongapped” of the config for AriocU or AriocP. gapped_args: these arguments in string form are passed literally to the “gapped” element of the config for AriocU or AriocP. nongapped_args: these arguments in string form are passed literally to the “nongapped” element of the config for AriocU or AriocP. x_args: these arguments in string form are passed literally to the “X” element of the config for AriocU or AriocP. max_gpus: an integer, specifying how many GPUs to use, per sample, during alignment. Generally, it is recommended to keep this value at 1 unless there are more GPUs available than samples in your dataset. manually_set_gpu: (true or false) a boolean specifying whether to have BiocMAP search for an available GPU and set CUDA_VISIBLE_DEVICES “manually” when performing alignment. Generally, this should be set to false on computing clusters managed by a job scheduler like SLURM or SGE, since typically a proper job scheduler handles these details on behalf of the user. By contrast, on a local machine (where typically there is no job scheduler), this variable should be set to true to ensure two samples don’t get aligned simultaneously on the same GPU (which often results in crashing due to insufficient memory). gpu_perc_usage_cutoff: a percentage, specified as an integer, of GPU utilization (as measured by nvidia-smi) below which a GPU is considered “unoccupied” and thus available to use during alignment with Arioc. This setting only applies if manually_set_gpu is true. 8.2.3 Miscellaneous settings kallisto_single_args: literal arguments to pass to ‘kallisto quant’ for single-end samples, as part of the process for estimating bisulfite conversion efficiency when using the ‘–with_lambda’ option. "],["software.html", "9 Software 9.1 Software Versions 9.2 Docker Images", " 9 Software BiocMAP makes use of several external software tools. The pipeline additionally supports the use of these tools via docker containers- this section also documents the docker images used in this mode. 9.1 Software Versions Here is the full list of software used by this pipeline: Software Version Command used by the pipeline Arioc 1.43 AriocE, AriocP, AriocU Bismark 0.23.0 bismark_prepare_genome, bismark_methylation_extractor, bismark2bedgraph, coverage2cytosine cmake latest cmake fastQC 0.11.8 fastqc htslib 1.12 (software library) libBigWig 0.4.6 (software library) java 8+ java kallisto 0.46.1 kallisto MethylDackel 0.5.2 MethylDackel extract nextflow &gt;=0.27.0 (tested with 20.01.0) nextflow R user-dependent Rscript samblaster v.0.1.26 samblaster samtools 1.10 samtools Trim Galore! 0.6.6 trim_galore 9.2 Docker Images The following image versions are used in this pipeline, when it is installed via the “docker” or “singularity” modes. These are automatically managed by BiocMAP. Image Tag Software libddocker/arioc 1.43 Arioc libddocker/bioc_kallisto 3.17 R 4.3.0, Bioconductor 3.17, Kallisto 0.46.1 libddocker/bismark 0.23.0 Bismark libddocker/filter_alignments 1.0 samtools 1.10, samblaster v.0.1.26 libddocker/kallisto 0.46.1 Kallisto libddocker/methyldackel 0.5.2 MethylDackel, samtools 1.10 libddocker/quality_and_trim 0.6.6 FastQC 0.11.8, Cutadapt (latest), Trim Galore 0.6.6 libddocker/ubuntu16.04_base 1_v3 Ubuntu Base "],["help.html", "10 Help 10.1 Common Errors 10.2 Resuming 10.3 Docker help", " 10 Help 10.1 Common Errors BiocMAP should be configured so that fundamental issues related to pipeline function do not arise. If you encounter an error and believe it to be a design flaw in BiocMAP, you can always submit a github issue. However, please take a look at the following common issues: A job/ process is not given enough memory or time: pipeline runs on large samples or datasets may require more memory or a higher time limit. When reported correctly, the pipeline will indicate an error status of 140 (for SGE or SLURM environments); however, memory issues can take many forms, and related error messages are not always clear. In this example case, the process FilterAlignments failed due to insufficient memory, but indicated a general error status (1): How Nextflow may report memory-related errors Attempt to provide the process more memory in your config. In this case the configuration for FilterAlignments looks like this (for SGE users): withName: FilterAlignments { cpus = 2 penv = &#39;local&#39; memory = 16.GB clusterOptions = &#39;-l h_fsize=800G&#39; } Note that disk space may also be the limitation. See the configuration section for more info. The pipeline keeps halting during alignment: In particular, the output log shows the error message \"No GPUs are available.\". This can have a few causes– if you are running the pipeline on an SGE/SLURM-based cluster, BiocMAP assumes that when GPUs are fully occupied, pending jobs will be held in queue, and will not be run until those resources become available. When this assumption is not met, some potential workarounds (which also apply to local runs/ without a cluster) are: Set the max_gpus variable in your config file for the first module to 1, if you’ve raised it above this default value. Increase the number of CPUs required for the AlignReads process in your config file (see “Specifying Options for your Cluster”. In particular, make sure the ratio of total CPUs available to this value exceeds the number of GPUs available. For example, if you are running the pipeline locally, have 2 GPUs and 8 CPU cores, set cpus = 4 in the AlignReads process. If the above solutions don’t work, a last-case fix could be to lower queueSize in the executor scope of your config file for the first module to the number of GPUs available to you. Note that this limits the number of concurrent processes, and can thus greatly slow pipeline execution. executor { name = &#39;sge&#39; queueSize = 8 // By default, this is set to 40 submitRateLimit = &#39;1 sec&#39; exitReadTimeout = &#39;40 min&#39; } Nextflow has trouble communicating with your cluster: a less common issue can occur on slower clusters, related to nextflow failing to poll your grid scheduler (like SGE or SLURM) for information it needs about the jobs that are running. This can show up in an error message like: ProcessFilterAlignments (Prefix: Sample_FE2P1_blood)terminated for an unknown reason -- Likely it has been terminated by the external system. We have found that raising the exitReadTimeout to a large value (such as 40 minutes) solves this issue, but consider raising it further if needed. executor { name = &#39;sge&#39; queueSize = 40 submitRateLimit = &#39;1 sec&#39; exitReadTimeout = &#39;40 min&#39; } 10.2 Resuming An important feature of BiocMAP (because it is based on nextflow) is the ability to resume pipeline execution if an error occurs for any reason. To resume, you must add the -resume flag to your “main” script, determined here, and rerun. Otherwise, the default is to restart the entire pipeline, regardless of how much progress was made! 10.3 Docker help For those who wish to use docker to manage BiocMAP software dependencies, we provide a brief set-up guide. Install docker A set of instructions for different operating systems are available on the Docker site. Create a docker group sudo addgroup docker Add user to docker group sudo usermod -aG docker &lt;your_user&gt; Checking installation Log out and log back in to ensure your user is running with the correct permissions. Test Docker installation by running: docker run hello-world "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
