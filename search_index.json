[["index.html", "[INSERT PIPELINE NAME HERE] Overview Cite [INSERT PIPELINE NAME HERE] R session information", " [INSERT PIPELINE NAME HERE] Nicholas J. Eagles Lieber Institute for Brain Development, Johns Hopkins Medical Campus Leonardo Collado-Torres Lieber Institute for Brain Development, Johns Hopkins Medical CampusCenter for Computational Biology, Johns Hopkins University lcolladotor@gmail.com Overview [INSERT PIPELINE NAME HERE] is a pair of Nextflow-based pipelines for processing raw whole genome bisulfite sequencing data into Bioconductor-friendly bsseq objects in R. We provide tips for achieving significant increases in throughput and customization [LINK TO MANUSCRIPT WHEN IT’S OUT], by implementing the earlier processing steps manually in place of the “first module” we provide in this repository. We recommend this manual approach to advanced users who handle large WGBS datasets or are particularly interested in performance. Otherwise, one can run the first module and second module in series for a complete WGBS processing workflow. Diagram representing the “conceptual” workflow traversed by [INSERT PIPELINE NAME HERE]. Here some nextflow processes are grouped together for simplicity; the exact processes traversed are enumerated here. The red box indicates the FASTQ files are inputs to the pipeline; green coloring denotes major output files from the pipeline; the remaining boxes represent computational steps. Yellow-colored steps are optional or not always performed; for example, lambda pseudoalignment is an optional step intended for experiments with spike-ins of the lambda bacteriophage. Finally, blue-colored steps are ordinary processes which occur on every pipeline execution. Cite [INSERT PIPELINE NAME HERE] We hope that [INSERT PIPELINE NAME HERE] will be useful for your research. Please use the following information to cite the workflow provided by this software. Thank you! [TODO] This is a project by the R/Bioconductor-powered Team Data Science at the Lieber Institute for Brain Development. R session information Details on the R version used for making this book. The source code is available at LieberInstitute/[INSERT PIPELINE NAME HERE]. ## Load the package at the top of your script library(&quot;sessioninfo&quot;) ## Reproducibility information print(&#39;Reproducibility information:&#39;) Sys.time() proc.time() options(width = 120) session_info() ## [1] &quot;Reproducibility information:&quot; ## [1] &quot;2021-07-20 13:54:03 UTC&quot; ## user system elapsed ## 0.453 0.113 0.494 ## ─ Session info ─────────────────────────────────────────────────────────────────────────────────────────────────────── ## setting value ## version R version 4.1.0 (2021-05-18) ## os Ubuntu 20.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz UTC ## date 2021-07-20 ## ## ─ Packages ─────────────────────────────────────────────────────────────────────────────────────────────────────────── ## package * version date lib source ## bookdown 0.22 2021-04-22 [1] RSPM (R 4.1.0) ## cli 3.0.0 2021-06-30 [2] RSPM (R 4.1.0) ## digest 0.6.27 2020-10-24 [2] RSPM (R 4.1.0) ## evaluate 0.14 2019-05-28 [2] RSPM (R 4.1.0) ## htmltools 0.5.1.1 2021-01-22 [1] RSPM (R 4.1.0) ## knitr 1.33 2021-04-24 [2] RSPM (R 4.1.0) ## magrittr 2.0.1 2020-11-17 [2] RSPM (R 4.1.0) ## rlang 0.4.11 2021-04-30 [2] RSPM (R 4.1.0) ## rmarkdown 2.9 2021-06-15 [1] RSPM (R 4.1.0) ## rstudioapi 0.13 2020-11-12 [2] RSPM (R 4.1.0) ## sessioninfo * 1.1.1 2018-11-05 [2] RSPM (R 4.1.0) ## stringi 1.7.2 2021-07-14 [2] RSPM (R 4.1.0) ## stringr 1.4.0 2019-02-10 [2] RSPM (R 4.1.0) ## withr 2.4.2 2021-04-18 [2] RSPM (R 4.1.0) ## xfun 0.24 2021-06-15 [2] RSPM (R 4.1.0) ## yaml 2.2.1 2020-02-01 [2] RSPM (R 4.1.0) ## ## [1] /usr/local/lib/R/host-site-library ## [2] /usr/local/lib/R/site-library ## [3] /usr/local/lib/R/library This book was last updated on 2021-07-20 13:54:03. "],["quick-start.html", "1 Quick Start 1.1 Setup 1.2 Configuration", " 1 Quick Start A brief guide to setting up [INSERT PIPELINE NAME HERE]. A more detailed and thorough guide is here. 1.1 Setup Clone the [INSERT PIPELINE NAME HERE] repository with git clone git@github.com:LieberInstitute/[INSERT PIPELINE NAME HERE].git Change directory into the repository with cd [INSERT PIPELINE NAME HERE] Users of the JHPCE cluster should run bash install_software.sh \"jhpce\". Otherwise, you can install everything required locally with bash install_software.sh \"local\". Note: JHPCE users must also make an edit to their ~/.bashrc files, described here 1.2 Configuration 1.2.1 Your “main” script The script you will use to run the pipeline depends on the system (“executor”) you wish to run the pipeline on, as well as which module you wish to run. Executor Module SGE cluster first SGE cluster second SLURM cluster first SLURM cluster second local machine first local machine second The JHPCE cluster first The JHPCE cluster second Options included in the main script should be modified as appropriate for the experiment. On SLURM and SGE clusters (including JHPCE), the main script should be submitted as a job (i.e. using sbatch or qsub). On local machines, the pipeline can be run interactively (i.e. bash run_pipeline_local.sh). 1.2.2 Your config file Your combination of “executor” (SLURM cluster, SGE cluster, or local) and “module” (first half or second half) determine the name of your configuration file. Find your file under [INSERT PIPELINE NAME HERE]/conf/. Executor Module Config Filename SGE cluster first first_half_sge.config SGE cluster second second_half_sge.config SLURM cluster first first_half_slurm.config SLURM cluster second second_half_slurm.config local machine first first_half_local.config local machine second second_half_local.config The JHPCE cluster first first_half_jhpce.config The JHPCE cluster second second_half_jhpce.config As an example, suppose you have access to a SLURM cluster, and wish to run the second module to process existing alignments. Your config file is then [INSERT PIPELINE NAME HERE]/conf/second_half_slurm.config. "],["setup-details.html", "2 Setup Details 2.1 Requirements 2.2 Installation 2.3 Run the Pipeline 2.4 Sharing the pipeline with many users", " 2 Setup Details 2.1 Requirements [INSERT PIPELINE NAME HERE] is designed for execution on Linux, and requires that the following be installed: Java 8 or later Python 3 (tested with 3.7.3), with pip R (tested with R 3.6-4.0) Additionally, installation makes use of GNU make and requires a C compiler, such as GCC. Both of these tools come by default with most Linux distributions. 2.2 Installation [INSERT PIPELINE NAME HERE] makes use of a number of different additional software tools. A bash script is provided to automatically install dependencies without the need for special (root) permissions. Local install: Installation is performed by running bash install_software.sh \"local\" from within the repository. This installs nextflow, several bioinformatics tools, R and packages, and sets up some test files. A full list of software used is here. The script install_software.sh builds each software tool from source, and hence relies on some common utilities which are often pre-installed in many unix-like systems: A C/C++ compiler, such as GCC or Clang The GNU make utility The makeinfo utility git, for downloading some software from their GitHub repositories The unzip utility Note: users at the JHPCE cluster do not need to worry about managing software via the above methods (required software is automatically available through modules). Simply run bash install_software.sh \"jhpce\" to install any missing R packages and set up some test files. Next, make sure you have the following lines added to your ~/.bashrc file: if [[ $HOSTNAME == compute-* ]]; then module use /jhpce/shared/jhpce/modulefiles/libd fi 2.2.1 Troubleshooting Some users may encounter errors during the installation process, particularly when installing software locally. We provide a list below of the most common installation-related issues. [INSERT PIPELINE NAME HERE] has been tested on: {TODO] 2.2.1.1 Required utilities are missing This is particularly common issue for users trying to get [INSERT PIPELINE NAME HERE] running on a local machine. We will assume the user has root privileges for the solutions suggested below. # On Debian or Ubuntu: sudo apt install [TODO] # On RedHat or CentOS: sudo yum install [TODO] 2.3 Run the Pipeline The “main” script used to run the pipeline depends on the environment you will run it on. 2.3.1 Run in a SLURM environment/ cluster (Optional) Adjust configuration: hardware resource usage, software versioning, and cluster option choices are specified in conf/first_half_slurm.config and conf/second_half_slurm.config. Modify the main script and run: the main scripts are run_first_half_slurm.sh and run_second_half_slurm.sh. Each script may be submitted to the cluster as a job (e.g. sbatch run_first_half_slurm.sh). See the full list of command-line options for other details about modifying the script for your use-case. To run the complete workflow, it is recommended to first submit run_first_half_slurm.sh, then monitor the output log run_first_half_slurm.log so that run_second_half_slurm.sh may be submitted when the log indicates the first half has completed. See here for Nextflow’s documentation regarding SLURM environments. 2.3.2 Run on a Sun Grid Engines (SGE) cluster (Optional) Adjust configuration: hardware resource usage, software versioning, and cluster option choices are specified in conf/first_half_sge.config and conf/second_half_sge.config. Modify the main script and run: the main scripts are run_first_half_sge.sh and run_second_half_sge.sh. Each script may be submitted to the cluster as a job (e.g. qsub run_first_half_sge.sh). See the full list of command-line options for other details about modifying the script for your use-case. To run the complete workflow, it is recommended to first submit run_first_half_sge.sh, then monitor the output log run_first_half_sge.log so that run_second_half_sge.sh may be submitted when the log indicates the first half has completed. See here for additional information on nextflow for SGE environments. 2.3.3 Run locally (Optional) Adjust configuration: hardware resource usage and other configurables are located in conf/first_half_local.config and conf/second_half_local.config. Modify the main script and run: the main scripts are run_first_half_local.sh and run_second_half_local.sh. After configuring options for your use-case (See the full list of command-line options), each script may be run interactively (e.g. bash run_first_half_local.sh). 2.3.4 Run on the JHPCE cluster (Optional) Adjust configuration: default configuration with thoroughly testing hardware resource specification is described within conf/first_half_jhpce.config and conf/second_half_jhpce.config. Modify the main script and run: the “main” scripts are run_first_half_jhpce.sh and run_second_half_jhpce.sh. Each script may be submitted to the cluster as a job (e.g. qsub run_first_half_jhpce.sh). See the full list of command-line options for other details about modifying the script for your use-case. To run the complete workflow, it is recommended to first submit run_first_half_jhpce.sh, then monitor the output log run_first_half_jhpce.log so that run_second_half_jhpce.sh may be submitted when the log indicates the first half has completed. 2.3.5 Example main script Below is a full example of a typical main script for the first module, modified from the run_first_half_jhpce.sh script. At the top are some cluster-specific options, recognized by SGE, the grid scheduler at the JHPCE cluster. These are optional, and you may consider adding appropriate options similarly, if you plan to use [INSERT PIPELINE NAME HERE] on a computing cluster. After the main command, nextflow first_half.nf, each command option can be described line by line: --sample \"paired\": input samples are paired-end --reference \"hg38\": these are human samples, to be aligned to the hg38 reference genome --input \"/users/neagles/wgbs_test\": /users/neagles/wgbs_test is a directory that contains the samples.manifest file, describing the samples. --output \"/users/neagles/wgbs_test/out\": /users/neagles/wgbs_test/out is the directory (which possibly exists already) where pipeline outputs should be placed. -profile jhpce: configuration of hardware resource usage, and more detailed pipeline settings, is described at conf/jhpce.config, since this is a run using the JHPCE cluster -w \"/fastscratch/myscratch/neagles/nextflow_work\": this is a nextflow-specific command option (note the single dash), telling [INSERT PIPELINE NAME HERE] that temporary files for the pipeline run can be placed under /fastscratch/myscratch/neagles/nextflow_work. --trim_mode \"force\": this optional argument instructs [INSERT PIPELINE NAME HERE] to trim all samples. Note there are alternative options. -profile first_half_jhpce: this line, which should typically always be included, tells [INSERT PIPELINE NAME HERE] to use conf/first_half_jhpce.config as the configuration file applicable to this pipeline run. #!/bin/bash #$ -l bluejay,mem_free=25G,h_vmem=25G,h_fsize=800G #$ -o ./run_first_half_jhpce.log #$ -e ./run_first_half_jhpce.log #$ -cwd module load nextflow export _JAVA_OPTIONS=&quot;-Xms8g -Xmx10g&quot; nextflow first_half.nf \\ --sample &quot;paired&quot; \\ --reference &quot;hg38&quot; \\ --input &quot;/users/neagles/wgbs_test&quot; \\ --output &quot;/users/neagles/wgbs_test/out&quot; \\ -w &quot;/fastscratch/myscratch/neagles/nextflow_work&quot; \\ --trim_mode &quot;force&quot; \\ -profile first_half_jhpce 2.4 Sharing the pipeline with many users A single installation of [INSERT PIPELINE NAME HERE] can be shared among potentially many users. New users can simply copy the appropriate “main” script (determined above) to a different desired directory, and modify the contents as appropriate for the particular experiment. Similarly, a single user can copy the “main” script and modify the copy whenever there is a new experiment/ set of samples to process, reusing a single installation of [INSERT PIPELINE NAME HERE] arbitrarily many times. Note It is recommended to use a unique working directory with the -w option for each experiment. This ensures: [INSERT PIPELINE NAME HERE] resumes from the correct point, if ever stopped while multiple users are running the pipeline Deleting the work directory (which can take a large amount of disk space) does not affect [INSERT PIPELINE NAME HERE] execution for other users or other experiments 2.4.1 Customizing execution for each user By default, all users will share the same configuration. This likely suffices for many use cases, but alternatively new configuration files can be created. Below we will walk through an example where a new user of a SLURM-based cluster wishes to use an existing [INSERT PIPELINE NAME HERE] installation to run the first module, but wants a personal configuration file to specify different annotation settings. Copy the existing configuration to a new file # Verify we are in the [INSERT PIPELINE NAME HERE] repository pwd # Create the new configuration file cp conf/first_half_slurm.config conf/my_first_half_slurm.config Modify the new file as desired Below we will change the GENCODE release to the older release 25, for human, via the gencode_version_human variable. params { //---------------------------------------------------- // Annotation-related settings //---------------------------------------------------- gencode_version_human = &quot;25&quot; // originally was &quot;34&quot;! gencode_version_mouse = &quot;M23&quot; anno_build = &quot;main&quot; // main or primary (main is canonical seqs only) See configuration for details on customizing [INSERT PIPELINE NAME HERE] settings. Add the new file as a “profile” This involves adding some code to nextflow.config, as shown below. profiles { // Here we&#39;ve named the new profile &quot;my_new_config&quot;, and pointed it to the // file &quot;conf/my_first_half_slurm.config&quot;. my_new_config { includeConfig &#39;conf/my_first_half_slurm.config&#39; } Reference the new profile in the “main” script Recall that new users should copy the “main” script and modify the copy as appropriate. In this case, we open a copy of the original run_first_half_slurm.sh: # At the nextflow command, we change the &#39;-profile&#39; argument at the bottom $ORIG_DIR/Software/bin/nextflow $ORIG_DIR/first_half.nf \\ --sample &quot;paired&quot; \\ --reference &quot;hg38&quot; \\ -profile my_new_config # this was changed from &quot;-profile first_half_slurm&quot;! "],["command-opts.html", "3 All Command Options 3.1 [INSERT PIPELINE NAME HERE] Options 3.2 Nextflow Options", " 3 All Command Options 3.1 [INSERT PIPELINE NAME HERE] Options Each of the below parameters, unless otherwise noted, applies to both the first and second module provided in [INSERT PIPELINE NAME HERE]. 3.1.1 Mandatory Parameters --sample “single” or “paired”: whether reads are single-end or paired-end --reference “hg38”, “hg19”, or “mm10”. The reference genome to be used for alignment and methylation extraction 3.1.2 Optional Parameters --all_alignments (First module only) Include this flag to signal Arioc to also write outputs for discondant, rejected, and unmapped reads. Sam files for each outcome are kept as pipeline outputs. In either case, only concordant reads are used for later processing (methylation extraction and beyond) --annotation The path to the directory containing reference-related files. Defaults to “./ref” (relative to the repository). If annotation files are not found here, the pipeline includes a step to build them. --custom_anno [label] Include this flag to indicate that the directory specified with --annotation [dir] includes user-provided annotation files to use instead of the default files. See the “Using custom annotation” section for more details. --input If using the first module, the path to the directory containing the “samples.manifest” file, or, if using the second module, the path to the directory containing the “rules.txt” file. Defaults to “./test” (relative to the repository) --output The path to the directory to store pipeline output files. Defaults to “./results” (relative to the repository) --trim_mode (First module only) Determines the conditions under which trimming occurs: “skip”: do not perform trimming on samples “adaptive”: [default] perform trimming on samples that have failed the FastQC “Adapter content” metric “force”: perform trimming on all samples --use_bme (Second module only) Include this flag to perform methylation-extraction-related processes with Bismark utilities, rather than the default of MethylDackel --with_lambda (Second module only) Include this flag if all samples have spike-ins with the lambda bacteriophage genome. Pseudoalignment will then be performed to estimate bisulfite conversion efficiency 3.2 Nextflow Options The nextflow command itself provides many additional options you may add to your “main” script. A few of the most commonly applicable ones are documented below. For a full list, type [path to nextflow] run -h- the full list does not appear to be documented at nextflow’s website. -w [path] Path to the directory where nextflow will place temporary files. This directory can fill up very quickly, especially for large experiments, and so it can be useful to set this to a scratch directory or filesystem with plenty of storage capacity. -resume Include this flag if pipeline execution halts with an error for any reason, and you wish to continue where you left off from last run. Otherwise, by default, nextflow will restart execution from the beginning. -with-report [filename] Include this to produce an html report with execution details (such as memory usage, completion details, and much more) N [email address] Sends email to the specified address to notify the user regarding pipeline completion. Note that nextflow relies on the sendmail tool for this functionality- therefore sendmail must be available for this option to work. "],["pipeline-overview.html", "4 Pipeline Overview 4.1 Preparation Steps 4.2 Main Workflow Steps: First Module 4.3 Main Workflow Steps: Second Module", " 4 Pipeline Overview Diagram representing the “conceptual” workflow traversed by [INSERT PIPELINE NAME HERE]. Here some nextflow processes are grouped together for simplicity; the exact processes traversed are enumerated below. The red box indicates the FASTQ files are inputs to the pipeline; green coloring denotes major output files from the pipeline; the remaining boxes represent computational steps. Yellow-colored steps are optional or not always performed; for example, preparing a particular set of annotation files occurs once and uses a cache for further runs. Finally, blue-colored steps are ordinary processes which occur on every pipeline execution. 4.1 Preparation Steps The following processes in the pipeline are done only once for a given configuration, and are skipped on all subsequent runs: 4.1.1 Downloading and preparing required annotation files PullReference: when using default annotation, this process pulls the genome fasta from GENCODE, possibly subsets to canonical sequences (see annotation builds, and saves to the directory specified by --annotation. This is the file against which FASTQ reads are aligned, and methylation is extracted. PrepareReference: when run as part of the first module: split the single FASTA file into individual files (for handling with Arioc) and write the configuration file later used by Arioc to encode these reference files. When run as part of the second module: runs bismark_genome_preparation with the --hisat option, in preparation for possible methylation extraction using Bismark (though note that MethylDackel is used instead by default!). EncodeReference: run AriocE to encode reference files for use with the Arioc aligner. This process requires a GPU. PrepareLambda when using the --with_lambda option as part of the second module, this process downloads the lambda bacteriophage genome and creates kallisto indices required for later estimation of bisulfite conversion efficiency. 4.2 Main Workflow Steps: First Module PreprocessInputs: Merge FASTQ files where specified in samples.manifest and create conveniently named soft links of each FASTQ file for internal use in [INSERT PIPELINE NAME HERE]. FastQC_Untrimmed: FASTQ files are ran through FastQC as a preliminary quality control measure. By default, the presence of “adapter content” determined in this process decides whether a given sample is trimmed in the next step. See the --trim_mode command option for modifying this default behavior. Trimming: See “FastQC_Untrimmed” above- FASTQ inputs are trimmed with Trim Galore! either based on adapter content from FastQC, or differently based on the --trim_mode [command option]. FastQC is also run as a confirmation step after trimming. For samples that are trimmed, post-trimming FastQC metrics are collected and later gathered into an R data frame; for all other samples, these metrics are collected from the FastQC run prior to any trimming. WriteAriocConfigs: Generate configurations file for use in AriocE, followed by AriocU or AriocP, using configuration settings specified in the appropriate config. Note two configuration files are created for each sample. EncodeReads: Run AriocE to encode each sample in preparation for the alignment step with Arioc. AlignReads: Align each sample to the reference genome using Arioc, creating at least one SAM file. FilterAlignments: Filter, sort, compress, and index alignments from Arioc. First, primary alignments with a MAPQ of at least 5 are kept, and duplicate alignments are removed using samblaster. The result is coordinate-sorted, compressed into BAM format, and this BAM file is indexed. MakeRules: A rules.txt file is created in the same directory as samples.manifest so that the second module can be easily run by the user after completion of the first module. 4.3 Main Workflow Steps: Second Module PreprocessInputs: parse rules.txt and samples.manifest, the inputs to the second module, so that all relevant FASTQ files and logs may be handled correctly in later steps. LambdaPseudo: when the --with_lambda option is specified, pseudoalignment to the lambda bacteriophage transcriptome is performed. Both the (overwhelmingly unmethylated) original transcriptome and then an “in-silico bisulfite-converted” version of the transcriptome are used in series. Successful map rates are used to estimate the entire experiment’s bisulfite conversion rate, a metric which is included in the final output R data frame. BME: when using the --use_bme option, methylation extraction is performed using bismark_methylation_extraction. Bismark2Bedgraph: when using the --use_bme option, the bismark2bedgraph utility from Bismark is run to generate intermediate bedGraph files, which are later used to generate text-based “cytosine reports”. Coverage2Cytosine: when using the --use_bme option, the coverage2cytosine utility from Bismark created text-based “cytosine reports”, which are later read into R to produce the final bsseq objects. MethylationExtraction: this process is run by default in place of the above Bismark-related processes, and performs methylation extraction with MethylDackel, ultimately generating “cytosine reports” comparable to those produced by the coverage2cytosine Bismark utility. ParseReports: logs from FastQC, any trimming, alignment, methylation extraction, and, if applicable, bisulfite conversion estimation, are parsed to extract key metrics into an R data frame. This data frame is produced as both a standalone .rda file, and included as part of the colData slots in each of the output bsseq objects. FormBsseqObjects: bsseq R objects are formed, one for each canonical chromosome and all samples, from information provided in the “cytosine reports” generated in either the MethylationExtraction or Coverage2Cytosine process. These are typically considered intermediary files, as the next process merges the many bsseq objects from this step into just a pair of final outputs, but for very large experiments, the objects from this step may be more reasonable to work with (to reduce memory requirements). MergeBsseqObjects: the final process in the pipeline, which combines all previous bsseq objects into just two objects- the first containing all cytosines in “CpG” context in the genome, and the other containing those in “CpH” context. The colData slot in each of these objects contain the metrics extracted in the ParseReports process, some of which might be useful covariates in potential downstream statistical analyses. See outputs for more information. "],["annotation.html", "5 Annotation 5.1 Default Annotation 5.2 Custom Annotation", " 5 Annotation [INSERT PIPELINE NAME HERE] can be run with hg38, hg19, or mm10 references. The pipeline has a default and automated process for pulling and building annotation-related files, but the user can opt to provide their own annotation as an alternative. Both of these options are documented below. Example annotation files below are the ones used with default configuration when hg38 reference is selected. A genome assembly fasta: the reference genome to align reads to, like the file here (but unzipped). Gene annotation gtf: containing transcript data, like the file here (but unzipped). The lambda transcriptome: for experiments utilizing spike-ins of the lambda bacteriophage genome, the transcriptome provided here is used (but unzipped). 5.1 Default Annotation [INSERT PIPELINE NAME HERE] uses annotation files provided by GENCODE. 5.1.1 Choosing a release With genome annotation constantly being updated, the user may want to use a particular GENCODE release. The configuration variables gencode_version_human and gencode_version_mouse control which GENCODE release is used for the human and mouse genomes, respectively. params { //---------------------------------------------------- // Annotation-related settings //---------------------------------------------------- gencode_version_human = &quot;34&quot; gencode_version_mouse = &quot;M23&quot; anno_build = &quot;main&quot; // main or primary (main is canonical seqs only) 5.1.2 Choosing a “build” Depending on the analysis you are doing, you may wish to only consider the reference chromosomes (for humans, the 25 sequences “chr1” through “chrM”) for alignment and methylation extraction. [INSERT PIPELINE NAME HERE] provides the option to choose from two annotation “builds” for a given release and reference, called “main” and “primary” (following the naming convention from GENCODE databases). The “main” build consists of only the canonical “reference” sequences for each species The “primary” build consists of the canonical “reference” sequences and additional scaffolds, as a genome primary assembly fasta from GENCODE would contain. See the variable annotation_build in your configuration file for making this selection for your pipeline run. 5.2 Custom Annotation You may wish to provide a genome FASTA (the reference genome to align reads to), such as the file here, in place of the automatically managed GENCODE files described in the above section. You must also add the --custom_anno [label] argument to your execution scripts, to specify you are using custom annotation files. The “label” is a string you want to include in filenames generated from the annotation files you provided. This is intended to allow the use of potentially many different custom annotations, assigned a unique and informative name you choose each time. This can be anything except an empty string (which internally signifies not to use custom annotation). "],["inputs.html", "6 Inputs 6.1 The samples.manifest File 6.2 The rules.txt File", " 6 Inputs The only input to the first module is a file called samples.manifest. This user-created file associates each FASTQ file with a path and ID, and allows the pipeline to automatically merge files if necessary. After creating the samples.manifest file, one must add the --input argument to the approriate execution script for the first module, and its value should be the path to the directory containing samples.manifest. For example, if the full path is /users/neagles/samples.manifest, the full argument in the execution script should be --input \"/users/neagles\". The second module requires the same samples.manifest file and an additional file called rules.txt. The latter specifies where relevant logs and the samples.manifest file are located. Similarly as in the first module, one must add the --input argument to the approriate execution script for the second module, but its value should be the path to the directory containing rules.txt. 6.1 The samples.manifest File 6.1.1 What samples.manifest should look like Each line in samples.manifest should have the following format: For a set of unpaired reads &lt;PATH TO FASTQ FILE&gt;(tab)&lt;optional MD5&gt;(tab)&lt;sample label/id&gt; For paired-end sets of reads &lt;PATH TO FASTQ 1&gt;(tab)&lt;optional MD5 1&gt;(tab)&lt;PATH TO FASTQ 2&gt;(tab)&lt;optional MD5 2&gt;(tab)&lt;sample label/id&gt; A line of paired-end reads could look like this: WGBS_sample1_read1.fastq 0 WGBS_sample1_read2.fastq 0 sample1 The MD5(s) on each line are for compatibility with a conventional samples.manifest structure, and are not explicitly checked in the pipeline (you may simply use 0s as in the above example). Paths must be long/full. If you have a single sample split across multiple files, you can signal for the pipeline to merge these files by repeating the sample label/id on each line of files to merge. A samples.manifest file cannot include both single-end and paired-end reads; separate pipeline runs should be performed for each of these read types. This is an example of a samples.manifest file for some paired-end samples. Note how the first sample “dm3” is split across more than one pair of files, and is to be merged: /scratch/dm3_file1_1.fastq 0 /scratch/dm3_file1_2.fastq 0 dm3 /scratch/dm3_file2_1.fastq 0 /scratch/dm3_file2_2.fastq 0 dm3 /scratch/sample_01_1.fastq.gz 0 /scratch/sample_01_2.fastq.gz 0 sample_01 /scratch/sample_02_1.fastq.gz 0 /scratch/sample_02_2.fastq.gz 0 sample_02 6.1.2 More details regarding inputs Input FASTQ files can have the following file extensions: .fastq, .fq, .fastq.gz, .fq.gz FASTQ files must not contain “.” characters before the typical extension (e.g. sample.1.fastq), since some internal functions rely on splitting file names by “.”. 6.1.3 Creating a manifest file In a common scenario, you may have a large number of FASTQ files in a single directory, for a given experiment. How can the samples.manifest file be constructed in this case? While the method you use is a matter of preference, we find it straightforward to write a small R script to generate the manifest. Suppose we have 3 paired-end samples, consisting of a total of 6 FASTQ files: /data/fastq/SAMPLE1_L001_R1_001.fastq.gz /data/fastq/SAMPLE1_L001_R2_001.fastq.gz /data/fastq/SAMPLE2_L002_R1_001.fastq.gz /data/fastq/SAMPLE2_L002_R2_001.fastq.gz /data/fastq/SAMPLE3_L003_R1_001.fastq.gz /data/fastq/SAMPLE3_L003_R2_001.fastq.gz The following script can generate the manifest appropriate for this experiment: # If needed, install the &#39;jaffelab&#39; GitHub-based package, which includes a # useful function for string manipulation remotes::install_github(&quot;LieberInstitute/jaffelab&quot;) library(&quot;jaffelab&quot;) fastq_dir &lt;- &quot;/data/fastq&quot; # We can take advantage of the uniform file naming convention to get the paths # of each mate in the pair, for every sample. Here we use a somewhat # complicated regular expression to match file names (to be sure we are # matching precisely the files we think we&#39;re matching), but this can be kept # simple if preferred. r1 &lt;- list.files(fastq_dir, &quot;.*_L00._R1_001\\\\.fastq\\\\.gz&quot;, full.names = TRUE) r2 &lt;- list.files(fastq_dir, &quot;.*_L00._R2_001\\\\.fastq\\\\.gz&quot;, full.names = TRUE) # We can form a unique ID for each sample by taking the portion of the path to # the first read preceding the lane and mate identifiers. The function &#39;ss&#39; is # a vectorized form of &#39;strsplit&#39;, handy for this task ids &lt;- ss(basename(r1), &quot;_L00&quot;) # Sanity check: there should be the same number of first reads as second reads stopifnot(length(R1) == length(R2)) # Prepare the existing sample information into the expected format (for now, # as a character vector where each element will be a line in # &#39;samples.manifest&#39;). We will simply use zeros for the optional MD5 sums. manifest &lt;- paste(r1, 0, r2, 0, ids, sep = &quot;\\t&quot;) # Write the manifest to a file (in this case, in the current working # directory) writeLines(manifest, con = &quot;samples.manifest&quot;) 6.2 The rules.txt File 6.2.1 What rules.txt should look like # An example &#39;rules.txt&#39; file manifest = /users/nick/samples.manifest sam = /users/nick/Arioc/[id]/sams/[id].cfu.sam arioc_log = /users/nick/Arioc/[id]/log/AriocP.[id].log xmc_log = /users/nick/Arioc/[id]/log/[id].cfu.XMC.log trim_report = /users/nick/trim_galore/[id]/[id].fastq.gz_trimming_report.txt fastqc_log_last = /users/nick/FastQC/[id]_trimmed_summary.txt fastqc_log_first = /users/nick/FastQC/[id]_untrimmed_summary.txt This file consists of several lines of key-value pairs, where keys and values are separated by an equals sign (“=”) and optionally spaces. Lines without “=” are ignored, and can be used as comments. The above example starts such lines with “#” for clarity. The required keys to include in a valid rules.txt file include “manifest”, “sam”, “arioc_log”, “trim_report”, and “fastqc_log_last”. The associated values for these keys are the paths to the samples.manifest file, the filtered/deduplicated alignment SAMs, verbose output logs from Arioc alignment, output logs from TrimGalore!, and logs from the latest or only run of FastQC (see the below point about optional keys), respectively. Note that users who plan to use MethylDackel for methylation extraction (the default!) should have coordinate-sorted and indexed BAM files ready, which are passed to the “sam” key using a glob expression. Otherwise, just one SAM file is expected per sample. Note this example for the former case: # Suppose we have 4 files across 2 samples: # /users/nick/alignments/id1_sorted.bam and /users/nick/alignments/id1_sorted.bam.bai # /users/nick/alignments/id2_sorted.bam and /users/nick/alignments/id2_sorted.bam.bai # We can point to these files using the &#39;sam&#39; key in &#39;rules.txt&#39;: sam = /users/nick/alignments/[id]_sorted.bam* Optional keys accepted in a rules.txt file include “xmc_log” and “fastqc_log_first”. Associated values are paths to logs from XMC (a utility that comes with the Arioc software), and *summary.txt logs from FastQC, respectively. The optional “fastqc_log_first” key can be used when the user runs FastQC twice (for example, before and after trimming) for at least one sample. In this case, “fastqc_log_first” refers to the pre-trimming run of FastQC. Typically, methylation extraction is done in the second module, and the “xmc_log” option is only included for compatibility with a previous WGBS processing approach. Information from these logs will be included in the R data frame output from the second module, if these logs are present and specified in rules.txt. Since exact paths are different between samples, including “[id]” in the value field in a line of rules.txt indicates that the path for any particular sample can be found by replacing “[id]” with its sample ID. Otherwise, paths are interpreted literally. For paired-end experiments, there will be two summary logs from FastQC. To correctly describe this in rules.txt, a glob expression may be used. As an example, suppose we have FastQC summary logs for a sample called sample1, given by the paths /some_dir/fastqc/sample1_1_summary.txt and /some_dir/fastqc/sample1_2_summary.txt. The following line in rules.txt would be appropriate: fastqc_log_last = /some_dir/fastqc/[id]_[12]_summary.txt Be careful with forming glob expressions! If we had a 10-sample experiment with sample names sample1 through sample10 but used a simpler asterisk \"*\" in place of the alternation \"_[12]\" in the above example, we would get an error! # This is wrong! When finding files for &quot;sample1&quot;, we&#39;d also match files for # &quot;sample10&quot;! fastqc_log_last = /some_dir/fastqc/[id]*_summary.txt The --input [dir] argument to the nextflow command in run_second_half_*.sh scripts specifies the directory containing the rules.txt file, and is a required argument. "],["outputs.html", "7 Outputs 7.1 Main Outputs 7.2 Intermediary Outputs", " 7 Outputs 7.1 Main Outputs 7.1.1 First Module [TODO] 7.1.2 Second Module [LIKELY WANT A FIGURE DEPICTING OUTPUT OBJECTS] 7.1.2.1 R Objects [TODO] 7.1.2.2 Metrics [TODO] 7.2 Intermediary Outputs [INSERT PIPELINE NAME HERE] generates a number of files along the process before producing the main outputs of interest in each module. Each of these “intermediary” files is described below. 7.2.1 First Module [TODO- needs editing and additional content] FastQC Outputs fastQC/ [trim_status]/[file_name]/*`: Outputs from FastQC. Here trim_status indicates when FastQC was performed: Untrimmed is before trimming, and Trimmed is after. file_name contains the sample name, and if applicable, the mate number. Alignment BAMs and Summaries alignment/ [sample_name].bam: The main alignment output from Hisat2 or optionally STAR in BAM format. In either case, unmapped reads are not included (different from the default behavior for HISAT2!). [sample_name]_align_summary.txt: The text-based alignment summary from Hisat2, if applicable. Note that metrics from these files are aggregated for the experiment, and so users likely will not need to check or process the original files manually. [sample_name]_STAR_alignment.log: Statistics from STAR alignment (if applicable) for a single sample, renamed from Log.final.out. Note that metrics from these files are aggregated for the experiment, and so users likely will not need to check or process the original files manually. bam_sort/[sample_name]_sorted.bamandbam_sort/[sample_name]_sorted.bam.bai`: Coordinate-sorted alignments and their corresponding indices. [sample_name]_unmapped_mate*.fastq: If using STAR with paired-end reads and the --unalign option, these two files (mates 1 and 2) are produced, including unmapped reads (this includes “discordant” reads). [sample_name]_discordant.fastq: If using HISAT2 (this is the default) and the --unalign option, this file is produced and includes discordant mappings. Trimmed FASTQ Files trimming/ [sample_name]_trimmed*.fastq: Trimmed FASTQ files, if applicable, from Trimmomatic 7.2.2 Second Module [TODO] "],["configuration.html", "8 Configuration 8.1 Specifying Options for your Cluster 8.2 [INSERT PIPELINE NAME HERE] settings", " 8 Configuration [INSERT PIPELINE NAME HERE] is designed to be highly customizable, yet need no configuration from a user wishing to rely on sensible default settings. Most configuration, including software settings, hardware resources such as memory and CPU use, and more, can be done in a single file determined here. Please note that the common, “major” options are specified in your main script, and not in the configuration file. The config is intended to hold the “finer details”, which are documented below. 8.1 Specifying Options for your Cluster In many cases, a user has access to a computing cluster which they intend to run [INSERT PIPELINE NAME HERE] on. If your cluster is SLURM or SGE-based, the pipeline is pre-configured with options you may be used to specifying (such as disk usage, time for a job to run, etc). However, these are straightforward to modify, should there be a need/desire. Common settings are described in detail below; however, a more comprehensive list of settings from nextflow can be found here. 8.1.1 Time The maximum allowed run time for a process, or step in the pipeline, can be specified. This may be necessary for users who are charged based on run time for their jobs. 8.1.1.1 Default for all processes The simplest change you may wish to make is to relax time constraints for all processes. The setting for this is here: executor { name = &#39;sge&#39; queueSize = 40 submitRateLimit = &#39;1 sec&#39; exitReadTimeout = &#39;40 min&#39; } process { cache = &#39;lenient&#39; time = 10.hour // this can be adjusted as needed While the syntax is not strict, some examples for properly specifying the option are time = '5m', time = '2h', and time = '1d' (minutes, hours, and days, respectively). 8.1.1.2 Specify per-process Time restrictions may also be specified for individual workflow steps. This can be done with the same syntax- suppose you wish to request 30 minutes for a given sample to be trimmed (note process names here). In the “process” section of your config, find the section labelled “withName: Trimming” as in the example here: withName: Trimming { cpus = 1 memory = 5.GB time = &#39;30m&#39; // specify a maximum run time of 30 minutes } 8.1.2 Cluster-specific options In some cases, you may find it simpler to directly specify options accepted by your cluster. For example, SGE users with a default limit on the maximum file size they may directly adjust the “h_fsize” variable as below: withName: FilterAlignments { cpus = 2 penv = &#39;local&#39; memory = 16.GB clusterOptions = &#39;-l h_fsize=800G&#39; } As with the time option, this can be specified per-process or for all processes. Any option recognized by your cluster may be used. 8.2 [INSERT PIPELINE NAME HERE] settings A number of variables in your config file exist to control choices about annotation to use, options passed to software tools, and more. These need not necessarily be changed, but allow more precise control over the pipeline if desired. Values for these variables may be changed in the “params” section of your config (near the top). A full descriptive list is provided below. 8.2.1 Annotation settings gencode_version_human: the GENCODE release to use for default (non-custom) annotation, when “hg38” or “hg19” references are used. A string, such as “32”. gencode_version_mouse: the GENCODE release to use for default (non-custom) annotation, when “mm10” reference is used. A string, such as “M23”. anno_build: controls which sequences are used in analysis with default (non-custom) annotation. “main” indicates only canonical reference sequences; “primary” includes additional scaffolds. 8.2.2 Arioc settings [TODO] 8.2.3 Miscellaneous settings [TODO: BRIEF DESCRIPTION] kallisto_single_args: [TODO] "],["software.html", "9 Software 9.1 Software Versions 9.2 Docker Images", " 9 Software [INSERT PIPELINE NAME HERE] makes use of several external software tools. The pipeline additionally supports the use of these tools via docker containers- this section also documents the docker images used in this mode. 9.1 Software Versions Here is the full list of software used by this pipeline: Software Version Command used by the pipeline Arioc 1.43 AriocE, AriocP, AriocU Bismark 0.23.0 bismark_prepare_genome, bismark_methylation_extractor, bismark2bedgraph, coverage2cytosine cmake latest cmake fastQC 0.11.8 fastqc hisat2 2.2.1 hisat2-build htslib 1.12 (software library) libBigWig 0.4.6 (software library) java 8+ java kallisto 0.46.1 kallisto MethylDackel 0.5.2 MethylDackel extract nextflow &gt;=0.27.0 (tested with 20.01.0) nextflow R user-dependent Rscript samblaster v.0.1.26 samtools 1.10 samtools Trim Galore! 0.6.6 trim_galore 9.2 Docker Images The following container versions are used in this pipeline. These are automatically managed by [INSERT PIPELINE NAME HERE]. Image Tag Software libddocker/r_3.6.1_bioc latest R and Bioconductor 3.10 libddocker/ubuntu16.04_base 1_v3 Ubuntu Base libddocker/kallisto 0.46.1 Kallisto [TODO] "]]
